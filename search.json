[
  {
    "objectID": "threads.html",
    "href": "threads.html",
    "title": "Coding for Performance",
    "section": "",
    "text": "Current computers almost all have multiple cores. We can divide the time it takes our code by up to the number of cores we have (but usually less) by writing multi-threaded code. Multi-threaded code performs multiple tasks at once with shared memory. Before you begin writing multi-threaded code, you should make sure your code isn’t already using all available cores. It is likely that the BLAS and Lapack libraries that Julia uses for linear algebra are multi-threaded. If you code is dominated by large matrix operations, it may already be using all available cores. In that case, there will not be much benefit from additional multi-threading.\nRead “The Basics of Single Node Parallel Computing” Rackauckus (2019) (Rackauckas 2019) .\nOnce we have decided that the code might benefit from multi-threading, we should look for loops (or other independent tasks) that can be multi-threaded. There is some overhead from creating threads and communicating among them. Multi-threading generally works best for loops where each iteration involves substantial work, and each iteration is independent of all others. The loops over grid points and bootstrap repetitions in gridbootstrap are perfect candidates. We don’t care about the order in which these loops get executed. The result of each iteration is (mostly) independent of all others.\nusing ARGridBootstrap, CodeTracking, Random, BenchmarkTools, Profile, ProfileCanvas\nfunction code_md(s)\n  println(\"```julia\\n\"*s*\"\\n```\\n\")\nend\nT = 200\ne = randn(T)\ny0 = 0\na = 0.9\ny = ar1_original(y0, a, e)\nest = b_est_original(y)\nαgrid = 0.84:(0.22/50):1.06\nnboot= 199\nwrapper(b_est) = function(x)\n  out=b_est(x)\n  (out.θ[3], out.se[3])\nend\n\n\ns=@code_string gridbootstrap_threaded(wrapper(b_est_original), (a, rng)-&gt;ar1_original(y0, a, est.e, n-&gt;rand(rng,1:(T-1),n)),αgrid, 2)\ncode_md(s)\nfunction gridbootstrap_threaded(estimator, simulator,\n                                grid::AbstractVector,\n                                nboot=199)\n  g = length(grid)\n  bootq = zeros(nboot, g)\n  ba    = zeros(nboot, g)\n  bootse = zeros(nboot,g)\n  #@threads for ak in 1:g\n  #  for j in 1:nboot\n  @threads for ind ∈ CartesianIndices(ba)\n    j = ind[1]\n    ak = ind[2]\n    (bootq[j,ak], bootse[j,ak]) = estimator(simulator(grid[ak],Random.TaskLocalRNG()))\n    ba[j,ak] = bootq[j,ak] - grid[ak]\n  end\n  ts = ba./bootse\n  (ba=ba, t=ts)\nend\n\n\n\nNow, let’s try multi-threading the original version of the code.\njulia&gt; using Base.Threads\n\njulia&gt; println(\"Single thread, original version\")\nSingle thread, original version\n\njulia&gt; @benchmark begin \n         (b,t) = gridbootstrap(wrapper(b_est_original), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 199)\n       end\nBenchmarkTools.Trial: 27 samples with 1 evaluation.\n Range (min … max):  175.276 ms … 199.619 ms  ┊ GC (min … max): 11.56% … 10.62%\n Time  (median):     186.845 ms               ┊ GC (median):    16.06%\n Time  (mean ± σ):   186.054 ms ±   4.257 ms  ┊ GC (mean ± σ):  15.72% ±  1.64%\n\n  ▁                    ▁ ▁  ▁▄ ▄▄▁█                              \n  █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁█▆█▁▁██▆████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆ ▁\n  175 ms           Histogram: frequency by time          200 ms &lt;\n\n Memory estimate: 219.44 MiB, allocs estimate: 345075.\njulia&gt; println(\"$(Threads.nthreads()) threads, original version\")\n40 threads, original version\n\njulia&gt; @benchmark begin \n         (b,t) = gridbootstrap_threaded(wrapper(b_est_original),\n                                        (a, rng)-&gt;ar1_original(y0, a, est.e, n-&gt;rand(rng,1:(T-1),n)),\n                                        αgrid, 199)\n       end\nBenchmarkTools.Trial: 32 samples with 1 evaluation.\n Range (min … max):  102.512 ms … 661.604 ms  ┊ GC (min … max):  0.00% … 81.64%\n Time  (median):     104.351 ms               ┊ GC (median):     0.00%\n Time  (mean ± σ):   158.058 ms ± 161.796 ms  ┊ GC (mean ± σ):  31.58% ± 24.16%\n\n  █                                                              \n  █▄▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▃ ▁\n  103 ms           Histogram: frequency by time          662 ms &lt;\n\n Memory estimate: 311.76 MiB, allocs estimate: 6383971.\nThe execution times are nearly identical on my computer. The reason is that the computation is dominated by the creation of X and multiplying X'*X and X'*y. These operations are already multi-threaded in the BLAS library being used. It is possible that first calling using LinearAlgebra; BLAS.set_num_threads(1) would improve the performance of the multi-threaded bootstrap.\n\n\n\njulia&gt; println(\"Single thread, fastest version\")\nSingle thread, fastest version\n\njulia&gt; estimator(y0=y0,e=est.e) = function(a)\n         out = simulate_estimate_arp(y0,a,e)\n         (out.θ[3], out.se[3])\n       end\nestimator (generic function with 3 methods)\n\njulia&gt; @benchmark  (b,t) = gridbootstrap(estimator(), a-&gt;a, αgrid, nboot)\nBenchmarkTools.Trial: 242 samples with 1 evaluation.\n Range (min … max):  18.752 ms … 48.284 ms  ┊ GC (min … max): 0.00% … 58.59%\n Time  (median):     19.663 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   20.719 ms ±  4.339 ms  ┊ GC (mean ± σ):  3.86% ±  9.76%\n\n  ▆█▇▆▃▁                                                       \n  ██████▇▆▄▁▁▁▁▁▆▄▄▁▁▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▄▁▁▁▁▁▁▁▁▇ ▆\n  18.8 ms      Histogram: log(frequency) by time      42.7 ms &lt;\n\n Memory estimate: 2.94 MiB, allocs estimate: 60904.\njulia&gt; println(\"$(Threads.nthreads()) threads, fastest version\")\n40 threads, fastest version\n\njulia&gt; estimator_threaded(y0=y0,e=est.e)=function(foo)\n         (a, rng) = foo\n         out=simulate_estimate_arp(y0,a,e,Val(1),()-&gt;rand(rng,1:length(e)))\n         (out.θ[3], out.se[3])\n       end\nestimator_threaded (generic function with 3 methods)\n\njulia&gt; @benchmark (bs, ts) = gridbootstrap_threaded(estimator_threaded(),(a,rng)-&gt;(a,rng), αgrid,nboot)\nBenchmarkTools.Trial: 1507 samples with 1 evaluation.\n Range (min … max):  1.620 ms … 141.653 ms  ┊ GC (min … max):  0.00% … 97.00%\n Time  (median):     2.524 ms               ┊ GC (median):     0.00%\n Time  (mean ± σ):   3.301 ms ±   8.923 ms  ┊ GC (mean ± σ):  18.36% ±  6.66%\n\n         ██▅▇▃                                                 \n  ▄▅▅▄▇▆▆█████████▇▅▅▄▅▃▄▃▃▂▂▂▂▃▂▃▂▂▃▂▂▂▂▁▂▂▁▂▂▂▁▂▁▂▂▁▂▂▂▁▂▂▂ ▃\n  1.62 ms         Histogram: frequency by time        6.71 ms &lt;\n\n Memory estimate: 2.97 MiB, allocs estimate: 61138.\nNotice how the speedup from using multiple threads is far less than number of cores. On my computer, the threaded version of the code is about 7 times faster, even though my computer has 40 “cores” (or 20 physical cores. My computer has 2 processors with 10 cores each, and each core is hyperthreaded into 2. The OS sees 40 processors, but half of them are sharing substantial resources). A speedup far less than the number of cores is typical. Creating and managing multiple threads creates some overhead. Moreover, cores must share various resources; most notably RAM and some cache."
  },
  {
    "objectID": "threads.html#multi-threaded-grid-bootstrap",
    "href": "threads.html#multi-threaded-grid-bootstrap",
    "title": "Coding for Performance",
    "section": "",
    "text": "s=@code_string gridbootstrap_threaded(wrapper(b_est_original), (a, rng)-&gt;ar1_original(y0, a, est.e, n-&gt;rand(rng,1:(T-1),n)),αgrid, 2)\ncode_md(s)\nfunction gridbootstrap_threaded(estimator, simulator,\n                                grid::AbstractVector,\n                                nboot=199)\n  g = length(grid)\n  bootq = zeros(nboot, g)\n  ba    = zeros(nboot, g)\n  bootse = zeros(nboot,g)\n  #@threads for ak in 1:g\n  #  for j in 1:nboot\n  @threads for ind ∈ CartesianIndices(ba)\n    j = ind[1]\n    ak = ind[2]\n    (bootq[j,ak], bootse[j,ak]) = estimator(simulator(grid[ak],Random.TaskLocalRNG()))\n    ba[j,ak] = bootq[j,ak] - grid[ak]\n  end\n  ts = ba./bootse\n  (ba=ba, t=ts)\nend"
  },
  {
    "objectID": "threads.html#some-libraries-are-already-multi-threaded",
    "href": "threads.html#some-libraries-are-already-multi-threaded",
    "title": "Coding for Performance",
    "section": "",
    "text": "Now, let’s try multi-threading the original version of the code.\njulia&gt; using Base.Threads\n\njulia&gt; println(\"Single thread, original version\")\nSingle thread, original version\n\njulia&gt; @benchmark begin \n         (b,t) = gridbootstrap(wrapper(b_est_original), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 199)\n       end\nBenchmarkTools.Trial: 27 samples with 1 evaluation.\n Range (min … max):  175.276 ms … 199.619 ms  ┊ GC (min … max): 11.56% … 10.62%\n Time  (median):     186.845 ms               ┊ GC (median):    16.06%\n Time  (mean ± σ):   186.054 ms ±   4.257 ms  ┊ GC (mean ± σ):  15.72% ±  1.64%\n\n  ▁                    ▁ ▁  ▁▄ ▄▄▁█                              \n  █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁█▆█▁▁██▆████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆ ▁\n  175 ms           Histogram: frequency by time          200 ms &lt;\n\n Memory estimate: 219.44 MiB, allocs estimate: 345075.\njulia&gt; println(\"$(Threads.nthreads()) threads, original version\")\n40 threads, original version\n\njulia&gt; @benchmark begin \n         (b,t) = gridbootstrap_threaded(wrapper(b_est_original),\n                                        (a, rng)-&gt;ar1_original(y0, a, est.e, n-&gt;rand(rng,1:(T-1),n)),\n                                        αgrid, 199)\n       end\nBenchmarkTools.Trial: 32 samples with 1 evaluation.\n Range (min … max):  102.512 ms … 661.604 ms  ┊ GC (min … max):  0.00% … 81.64%\n Time  (median):     104.351 ms               ┊ GC (median):     0.00%\n Time  (mean ± σ):   158.058 ms ± 161.796 ms  ┊ GC (mean ± σ):  31.58% ± 24.16%\n\n  █                                                              \n  █▄▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▃ ▁\n  103 ms           Histogram: frequency by time          662 ms &lt;\n\n Memory estimate: 311.76 MiB, allocs estimate: 6383971.\nThe execution times are nearly identical on my computer. The reason is that the computation is dominated by the creation of X and multiplying X'*X and X'*y. These operations are already multi-threaded in the BLAS library being used. It is possible that first calling using LinearAlgebra; BLAS.set_num_threads(1) would improve the performance of the multi-threaded bootstrap."
  },
  {
    "objectID": "threads.html#multi-threading-where-it-matters",
    "href": "threads.html#multi-threading-where-it-matters",
    "title": "Coding for Performance",
    "section": "",
    "text": "julia&gt; println(\"Single thread, fastest version\")\nSingle thread, fastest version\n\njulia&gt; estimator(y0=y0,e=est.e) = function(a)\n         out = simulate_estimate_arp(y0,a,e)\n         (out.θ[3], out.se[3])\n       end\nestimator (generic function with 3 methods)\n\njulia&gt; @benchmark  (b,t) = gridbootstrap(estimator(), a-&gt;a, αgrid, nboot)\nBenchmarkTools.Trial: 242 samples with 1 evaluation.\n Range (min … max):  18.752 ms … 48.284 ms  ┊ GC (min … max): 0.00% … 58.59%\n Time  (median):     19.663 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   20.719 ms ±  4.339 ms  ┊ GC (mean ± σ):  3.86% ±  9.76%\n\n  ▆█▇▆▃▁                                                       \n  ██████▇▆▄▁▁▁▁▁▆▄▄▁▁▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▄▁▁▁▁▁▁▁▁▇ ▆\n  18.8 ms      Histogram: log(frequency) by time      42.7 ms &lt;\n\n Memory estimate: 2.94 MiB, allocs estimate: 60904.\njulia&gt; println(\"$(Threads.nthreads()) threads, fastest version\")\n40 threads, fastest version\n\njulia&gt; estimator_threaded(y0=y0,e=est.e)=function(foo)\n         (a, rng) = foo\n         out=simulate_estimate_arp(y0,a,e,Val(1),()-&gt;rand(rng,1:length(e)))\n         (out.θ[3], out.se[3])\n       end\nestimator_threaded (generic function with 3 methods)\n\njulia&gt; @benchmark (bs, ts) = gridbootstrap_threaded(estimator_threaded(),(a,rng)-&gt;(a,rng), αgrid,nboot)\nBenchmarkTools.Trial: 1507 samples with 1 evaluation.\n Range (min … max):  1.620 ms … 141.653 ms  ┊ GC (min … max):  0.00% … 97.00%\n Time  (median):     2.524 ms               ┊ GC (median):     0.00%\n Time  (mean ± σ):   3.301 ms ±   8.923 ms  ┊ GC (mean ± σ):  18.36% ±  6.66%\n\n         ██▅▇▃                                                 \n  ▄▅▅▄▇▆▆█████████▇▅▅▄▅▃▄▃▃▂▂▂▂▃▂▃▂▂▃▂▂▂▂▁▂▂▁▂▂▂▁▂▁▂▂▁▂▂▂▁▂▂▂ ▃\n  1.62 ms         Histogram: frequency by time        6.71 ms &lt;\n\n Memory estimate: 2.97 MiB, allocs estimate: 61138.\nNotice how the speedup from using multiple threads is far less than number of cores. On my computer, the threaded version of the code is about 7 times faster, even though my computer has 40 “cores” (or 20 physical cores. My computer has 2 processors with 10 cores each, and each core is hyperthreaded into 2. The OS sees 40 processors, but half of them are sharing substantial resources). A speedup far less than the number of cores is typical. Creating and managing multiple threads creates some overhead. Moreover, cores must share various resources; most notably RAM and some cache."
  },
  {
    "objectID": "index.html#functions",
    "href": "index.html#functions",
    "title": "ARGridBootstrap.jl",
    "section": "Functions",
    "text": "Functions\n\nARGridBootstrap.ar1_original\nARGridBootstrap.argridbootstrap_gpu\nARGridBootstrap.argridkernel!\nARGridBootstrap.b_est_mldivide\nARGridBootstrap.b_est_nox\nARGridBootstrap.b_est_original\nARGridBootstrap.b_est_stride\nARGridBootstrap.gridbootstrap\nARGridBootstrap.gridbootstrap_threaded\nARGridBootstrap.invsym\nARGridBootstrap.oneto\nARGridBootstrap.rngarray\nARGridBootstrap.simulate_estimate_arp\nARGridBootstrap.simulate_estimate_arp_lv\n\n\n\n\nAR\n# ARGridBootstrap.ar1_original — Function.\nar1_original(y0, a, e, rindex=T-&gt;rand(1:length(e), T))\nSimulate AR1 model by sampling errors from e with replacement.\ny[t] = a*y[t-1] + ϵ[t]\nArguments\n\ny0: initial value for y\na: AR parameter\ne: values of for error term. ϵ = e[rindex(T)]]\nrindex function that returns random index in 1:length(e)\n\nReturns\n\ny: vector of length T = length(e)\n\nsource\n# ARGridBootstrap.b_est_mldivide — Method.\nb_est_mldivide(y)\nEstimate AR(1) model with intercept and time trend.\ny[t] = θ[0] + θ[1]t + θ[2]y[t-1] + e[t]\nArguments\n\ny: vector\n\nReturns\n\nθ: estimated coefficients\nse: standard errors\ne: residuals\n\nsource\n# ARGridBootstrap.b_est_nox — Method.\nb_est_nox(y)\nEstimate AR(1) model with intercept and time trend.\ny[t] = θ[0] + θ[1]t + θ[2]y[t-1] + e[t]\nArguments\n\ny: vector\n\nReturns\n\nθ: estimated coefficients\nse: standard errors\ne: residualas\n\nsource\n# ARGridBootstrap.b_est_original — Method.\nb_est_original(y)\nEstimate AR(1) model with intercept and time trend\ny[t] = θ[0] + θ[1]t + θ[2]y[t-1] + e[t]\nArguments\n\ny: vector\n\nReturns\n\nθ: estimated coefficients\nse: standard errors\ne: residuals\n\nsource\n# ARGridBootstrap.b_est_stride — Method.\nb_est_stride(y)\nEstimate AR(1) model with intercept and time trend.\ny[t] = θ[0] + θ[1]t + θ[2]y[t-1] + e[t]\nArguments\n\ny: vector\n\nReturns\n\nθ: estimated coefficients\nse: standard errors\ne: residualas\n\nsource\n# ARGridBootstrap.invsym — Method.\nfast inverse function for statically sized 3x3 StrideArray\nsource\n# ARGridBootstrap.oneto — Method.\noneto(Val(N)) Creates at a compile time the tuple (1, 2, …, N)\nsource\n# ARGridBootstrap.simulate_estimate_arp — Method.\nsimulate_estimate_arp(y0, a, e, ar::Val{P}, rindex=T-&gt;rand(1:length(e),T))\nSimulates and estimates an AR(P) model. y is simulated as\ny[t] = a*y[t-1] + ϵ[t]\nand the estimate of θ from\ny[t] = θ[1] + θ[2]t + θ[3] y[t-1] + … + θ[P] y[t-P] + u[t]\nis computed.\nArguments\n\ny0 initial value of y\na AR(1) parameter\ne error terms to sample from ϵ[t] = e[rindex(1)]\nar::Val{P} order of autoregressive model to estimate\nrindex function that returns random index in 1:length(e)\n\nReturns\n\nθ estimated coefficients\nse standard errors\n\nsource\n# ARGridBootstrap.simulate_estimate_arp_lv — Method.\nsimulate_estimate_arp_lv(y0, a, e, ar::Val{P}, rindex=T-&gt;rand(1:length(e),T))\nSimulates and estimates an AR(P) model. Uses LoopVectorization.jl to produce fast code.\ny is simulated as\ny[t] = a*y[t-1] + ϵ[t]\nand the estimate of θ from\ny[t] = θ[1] + θ[2]t + θ[3] y[t-1] + … + θ[P] y[t-P] + u[t]\nis computed.\nArguments\n\ny0 initial value of y\na AR(1) parameter\ne error terms to sample from ϵ[t] = e[rindex(1)]\nar::Val{P} order of autoregressive model to estimate\nrindex function that returns random index in 1:length(e)\n\nReturns\n\nθ estimated coefficients\nse standard errors\n\nsource\n\n\n\n\nGrid Bootstrap\n# ARGridBootstrap.argridbootstrap_gpu — Method.\nargridbootstrap_gpu(e; αgrid = 0.84:(0.22/20):1.06,\n                      nboot=199, RealType = Float32)\nComputes grid bootstrap estimates for an AR(1) model.\nFor each α ∈ grid, repeatedly simulate data with parameter α and then compute an estimate.\nArguments\n\ne vector error terms that will be resampled with replacement to generate bootstrap sample\ngrid grid of parameter values. For each value, nboot datasets will be simulated and estimates computed.\nnboot\nRealType type of numbers for GPU computation. On many GPUs, Float32 will have better performance than Float64.\n\nReturns\n\nba hatα - α for each grid value and simulated dataset\nt t-stat for each grid value and simulated dataset\n\nsource\n# ARGridBootstrap.argridkernel! — Method.\nargridkernel!(ba,bootq, bootse, ar::Val{P}, e, ei, αgrid)\nGPU kernel for simulation and estimation of AR(P) model.\nArguments (modified on return)\n\nba: nboot × ngrid array. Will be filled with bootstrap estimates of α grid values of true α\nbootq: nboot × ngrid array. Will be filled with bootstrap estimates of α\nbootse: nboot × ngrid array. Will be filled with standard errors of α for each bootstrap repetition\n\nArguments (not modified)\n\nar::Val{P} : autoregressive order for estimation. Simulated model will always be AR(1) with 0 intercept and time trend, but estimation will use an AR(P) model with intercept and time trend. Only the AR(1) parameter estimate is included in ba, bootq, and bootse.\ne : error terms to draw with replacement\nei : nboot × ngrid × length(e) array of indices of e to use to generate bootstrap sample1\nαgrid : length ngrid values of AR(1) parameter to perform bootstrap on.\n\nReturns nothing, but modifies in place ba, bootq, and bootse\nsource\n# ARGridBootstrap.gridbootstrap — Function.\ngridbootstrap(estimator, simulator,\n              grid::AbstractVector,\n              nboot=199)\nComputes grid bootstrap estimates a single parameter model.\nFor each α ∈ grid, repeatedly simulate data with parameter α and then compute an estimate.\nArguments\n\nestimator function of output of simulator that returns a 2-tuple containing an estimate of α and its standard error.\nsimulator function that given α simulates data that can be used to estimate α\ngrid grid of parameter values. For each value, nboot datasets will be simulated and estimates computed.\nnboot\n\nReturns\n\nba hatα - α for each grid value and simulated dataset\nt t-stat for each grid value and simulated dataset\n\nsource\n# ARGridBootstrap.gridbootstrap_threaded — Function.\ngridbootstrap_threaded(estimator, simulator,\n                grid::AbstractVector,\n                nboot=199, rng=rngarray(nthreads())\nComputes grid bootstrap estimates a single parameter model.\nMultithreaded version.\nFor each α ∈ grid, repeatedly simulate data with parameter α and then compute an estimate.\nArguments\n\nestimator function of output of simulator that returns a 2-tuple containing an estimate of α and its standard error.\nsimulator function that given α and rng, simulates data that can be used to estimate α\ngrid grid of parameter values. For each value, nboot datasets will be simulated and estimates computed.\nnboot number of bootstrap simulations per grid point\n\nReturns\n\nba hatα - α for each grid value and simulated dataset\nt t-stat for each grid value and simulated dataset\n\nsource\n# ARGridBootstrap.rngarray — Method.\nrngarray(n)\nCreate n rng states that will not overlap for 10^20 steps.\nNote: this will be unneeded in Julia 1.3 when thread-safe RNG is included.\nsource"
  },
  {
    "objectID": "argridboot.html",
    "href": "argridboot.html",
    "title": "Coding for Performance",
    "section": "",
    "text": "Today we will look into some methods to improve the speed of our code. Although speed is sometimes important, never forget that speed should be low on your list of priorities when writing code. You should prioritize correctness and maintainability ahead of performance. Nonetheless, performance does matter for some problems.\nIf you have not already, be sure to read the Peformance Tips section of Julia Docs.\nAlso, read Rackauckas’s notes on “Optimizing Serial Code.” (Rackauckas 2019)."
  },
  {
    "objectID": "argridboot.html#initial-benchmark-and-profile",
    "href": "argridboot.html#initial-benchmark-and-profile",
    "title": "Coding for Performance",
    "section": "Initial Benchmark and Profile",
    "text": "Initial Benchmark and Profile\nbenchorig=@benchmark (b,t) = gridbootstrap(wrapper(b_est_original), a-&gt;ar1_original(y0, a, est.e),\n                             αgrid, nboot)\nBenchmarkTools.Trial: 27 samples with 1 evaluation.\n Range (min … max):  173.543 ms … 206.242 ms  ┊ GC (min … max): 11.60% … 10\n.07%\n Time  (median):     187.021 ms               ┊ GC (median):    16.60%\n Time  (mean ± σ):   187.754 ms ±   6.373 ms  ┊ GC (mean ± σ):  16.05% ±  1\n.77%\n\n                 ▃  ▃    █  █         ▃                          \n  ▇▁▁▁▁▁▁▁▁▇▁▁▁▇▁█▁▁█▁▁▇▇█▇▇█▁▇▇▁▇▁▇▁▁█▇▇▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇ ▁\n  174 ms           Histogram: frequency by time          206 ms &lt;\n\n Memory estimate: 219.44 MiB, allocs estimate: 345075.\nTo make code faster, we should begin by profiling.\n# only to make profile results show nicely in generated html\n# for interactive use, just use @profview in place of @profile\nfunction profilehtmlstring() \n  buf = IOBuffer()\n  show(buf, MIME(\"text/html\"), ProfileCanvas.view(Profile.fetch()))\n  s=String(take!(buf))\n  println(\"\\n\\n\"*s*\"\\n\")\nend\nprofilehtmlstring (generic function with 1 method)\nusing Profile, ProfileCanvas\nProfile.clear();\nProfile.init(n=10^7,delay=0.0001);\n@profile (b,t) = gridbootstrap(wrapper(b_est_original), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 999)\nprofilehtmlstring()\n\n\n\n\nThe profiler works very simply. Every 0.0001 seconds, the line of code being executed gets recorded. There are then various tools for printing and visualizing the results. The @profview macro shows a flame graph. The default view might be a bit strange. The base of the flame graph often includes Julia’s repl and various other things that can be ignored. If you click on the graph, it will zoom in. You can use mouse wheel up to zoom back out.\n\n\n\n\n\n\nTip\n\n\n\nIn VSCode, you can just use @profview in place of @profile and the profile flamegraph will open in a panel within VSCode."
  },
  {
    "objectID": "argridboot.html#removing-redudant-operations",
    "href": "argridboot.html#removing-redudant-operations",
    "title": "Coding for Performance",
    "section": "Removing Redudant Operations",
    "text": "Removing Redudant Operations\n\n\n\n\n\n\nWarning\n\n\n\nThe following was true on an older version of Julia, but now there are no gains from eliminating inv.\n\n\nFrom the output (these numbers can vary quite a bit from run to run), we see there were 640 ticks in gridbootstrap_original (exact numbers will vary on each execution, but relative ones should be similar), and almost all of these occurred within inv. If we want the code to be faster, we should focus on these lines. Calling both inv and \\ is redundant; we should combine these computations.\ns = @code_string b_est_mldivide(y)\ncode_md(s)\nfunction b_est_mldivide(yin)\n  T = length(yin)\n  x = [ones(T-1) 2:T yin[1:(T-1)]]\n  y = yin[2:T]\n  tmp = x'*x \\ [x'*y I]\n  θ = tmp[:,1]\n  ixx = tmp[:,2:4]\n  e = y - x*θ\n  se = sqrt.(diag(ixx *(e'*e))./(T-4))\n  (θ=θ,se=se,e=e)\nend\nbenchml=@benchmark (b,t) = gridbootstrap(wrapper(b_est_mldivide), a-&gt;ar1_original(y0, a, est.e),\n                             αgrid, nboot)\nBenchmarkTools.Trial: 22 samples with 1 evaluation.\n Range (min … max):  219.171 ms … 245.086 ms  ┊ GC (min … max):  9.30% … 8.\n81%\n Time  (median):     228.146 ms               ┊ GC (median):    13.77%\n Time  (mean ± σ):   228.416 ms ±   5.812 ms  ┊ GC (mean ± σ):  12.81% ± 1.\n94%\n\n           █▃         ▃▃    ▃                                    \n  ▇▁▁▁▁▇▁▁▁██▁▁▁▇▁▇▁▁▁██▁▁▁▇█▁▇▁▁▁▇▁▇▇▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇ ▁\n  219 ms           Histogram: frequency by time          245 ms &lt;\n\n Memory estimate: 210.46 MiB, allocs estimate: 517608.\nFrom this, we get a speedup by about a factor of 4 on my computer. This used to make a big difference, but no longer seems to matter."
  },
  {
    "objectID": "argridboot.html#reducing-allocations",
    "href": "argridboot.html#reducing-allocations",
    "title": "Coding for Performance",
    "section": "Reducing Allocations",
    "text": "Reducing Allocations\nProfile.clear()\n@profile (b,t) = gridbootstrap(wrapper(b_est_mldivide), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 999)\nprofilehtmlstring()\n\n\n\n\nNow, the most time consuming parts of the code are, unsurprisingly, the call to \\, and, perhaps surprisingly, hcat from creating x. Allocating and copying memory is relatively slow. The creation of x involves both.\n\nCaching Intermediate Arrays\nOne option is to preallocate an arrays and reuse them. The struct bEstCache does this.\nb_est_cache = ARGridBootstrap.bEstCached(T-1)\nbenchcache=@benchmark gridbootstrap(wrapper(b_est_cache), a-&gt;ar1_original(y0, a, est.e), \n                                    αgrid, nboot)\nBenchmarkTools.Trial: 30 samples with 1 evaluation.\n Range (min … max):  161.391 ms … 184.374 ms  ┊ GC (min … max):  5.74% … 4.\n69%\n Time  (median):     167.982 ms               ┊ GC (median):    10.67%\n Time  (mean ± σ):   168.162 ms ±   4.797 ms  ┊ GC (mean ± σ):  10.23% ± 1.\n53%\n\n  ▃▃     ▃   ▃█    █    ▃ █   ▃                                  \n  ██▁▁▇▁▁█▁▁▇██▇▁▁▁█▁▁▁▇█▁█▁▇▇█▁▁▁▇▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇ ▁\n  161 ms           Histogram: frequency by time          184 ms &lt;\n\n Memory estimate: 143.71 MiB, allocs estimate: 162394.\nProfile.clear()\n@profview (b,t) = gridbootstrap(wrapper(b_est_cache), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 999)\nprofilehtmlstring()\n\n\n\n\n\n\n\nEliminating Intermediate Arrays\nBetter yet, we can avoid creating x by just accumulating \\(X'y\\) and \\(X'X\\) in a loop.\ns = @code_string b_est_nox(y)\ncode_md(s)\nfunction b_est_nox(yin; xx_xy!::F=xx_xy!, resids!::FR=resids!) where {F&lt;:Function, FR&lt;:Function}\n  T = length(yin)\n  xx = @MMatrix zeros(eltype(yin),3,3)\n  xy = @MVector zeros(eltype(yin),3)\n  xx_xy!(xx,xy,yin)\n  ixx = inv(xx)\n  θ = ixx * xy\n  e = similar(yin,T-1)\n  resids!(e,yin,θ)\n  se = sqrt.(diag(ixx *(e'*e))./(T-4))\n  (θ=θ,se=se,e=e)\nend\nWe put the two main loops into separate functions both for organization and to allow us to focus on optimizing these loops below.\ns=@code_string ARGridBootstrap.xx_xy!(zeros(3,3),zeros(3),y)\ncode_md(s)\n@inline function xx_xy!(xx,xy,yin)\n  T = length(yin)\n  xx .= zero(eltype(xx))\n  xy .= zero(eltype(xy))\n  @inbounds @simd for t in 2:T\n    xx[1,3] += yin[t-1]\n    xx[2,3] += t*yin[t-1]\n    xx[3,3] += yin[t-1]^2\n    xy[1] += yin[t]\n    xy[2] += t*yin[t]\n    xy[3] += yin[t-1]*yin[t]\n  end\n  xx[1,1] = T-1 # = 1'*1\n  xx[1,2] = xx[2,1] = (T+1)*T/2 - 1 # sum(p+1:T)\n  xx[2,2] = (2*(T)+1)*(T)*(T+1)/6 - 1 # sum((p+1:T).^2)\n  xx[3,1] = xx[1,3]\n  xx[3,2] = xx[2,3]\n  nothing\nend\ns=@code_string ARGridBootstrap.resids!(zeros(length(y)-1),y,zeros(3))\ncode_md(s)\n@inline function resids!(e, yin, θ)\n  T = length(yin)\n  @inbounds @simd for t in 2:T\n    e[t-1] = yin[t] - θ[1] - θ[2]*t - θ[3]*yin[t-1]\n  end\n  nothing\nend\nbenchnox=@benchmark (b,t) = gridbootstrap(wrapper(b_est_nox), a-&gt;ar1_original(y0, a, est.e),\n                             αgrid, nboot)\nBenchmarkTools.Trial: 79 samples with 1 evaluation.\n Range (min … max):  51.291 ms … 77.352 ms  ┊ GC (min … max):  0.00% … 0.00\n%\n Time  (median):     64.397 ms              ┊ GC (median):    18.95%\n Time  (mean ± σ):   63.800 ms ±  3.303 ms  ┊ GC (mean ± σ):  18.02% ± 4.70\n%\n\n                                          ▄ █    ▂▄▄▅ ▁        \n  ▃▃▁▃▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅████▅▅▅████▆█▁▅▁▃▃▃ ▁\n  51.3 ms         Histogram: frequency by time        67.5 ms &lt;\n\n Memory estimate: 71.70 MiB, allocs estimate: 101499.\nWe have further cut the time by a factor of two. However, this performance optimization has been costly in terms of readability and extensibility of our code. If we wanted to fit an AR(p) model instead of AR(1), the b_est_nox function would be more difficult to modify than the b_est_mldivide version.\nWe additionally gained some performance by using mutable StaticArrays to hold \\(X'X\\) and \\(X'y\\).\nxx = zeros(3,3)\nxy = zeros(3)\n@benchmark ARGridBootstrap.xx_xy!(xx,xy,y)\nBenchmarkTools.Trial: 10000 samples with 178 evaluations.\n Range (min … max):  574.433 ns …  1.254 μs  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     602.685 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   616.940 ns ± 41.532 ns  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n         █▂        ▄    ▁▁                                     ▁\n  ▃▁▁▁▁▁███▃▄▃▅▁▃▄▆███████▅▅▅▄▇▇▇▇▇███▇██▇▆▆▇▆▇▆▆▆▅▆▆▆▆▆▅▅▆▅▆▆ █\n  574 ns        Histogram: log(frequency) by time       800 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\nusing StaticArrays\nxx = @MMatrix zeros(3,3)\nxy = @MVector zeros(3)\n@benchmark ARGridBootstrap.xx_xy!(xx,xy,y)\nBenchmarkTools.Trial: 10000 samples with 181 evaluations.\n Range (min … max):  587.293 ns …  1.028 μs  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     591.155 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   598.737 ns ± 29.527 ns  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n  ▂█▄            ▂                                             ▁\n  ███▄▄▁▄▁▁▁▃▃▁▃▆█▇▆███▇█▇▆▅▅▅▅▃▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▄▆▅▅▆▄▅▄▅▃▄▅ █\n  587 ns        Histogram: log(frequency) by time       747 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0."
  },
  {
    "objectID": "argridboot.html#loopvectorization.jl",
    "href": "argridboot.html#loopvectorization.jl",
    "title": "Coding for Performance",
    "section": "LoopVectorization.jl",
    "text": "LoopVectorization.jl\nAs mentioned above, the Julia compiler tries to automactically use SIMD instructions when it is safe to do so. SIMD instructions often change the order of operations, and since floating point math is not exactly commutative. The compiler tries to avoid reordering operations, but this often prevents SIMD use. The macro @simd tells the compiler to not worry about reordering operations and insert SIMD instructions more aggresively. Still, there are some SIMD operations that the compiler will not insert automatically.\nThe LoopVectorization.jl package defines a macro, @turbo that more aggresively inserts SIMD instructions. This can make a large difference for some loops and broadcasts.\nusing StaticArrays\ns = @code_string simulate_estimate_arp(y0,a,e)\ncode_md(s)\nfunction simulate_estimate_arp(y0, a, e, ar::Val{P}=Val(1),\n                               rindex=()-&gt;rand(1:length(e))) where P\n  T = length(e)\n  length(a)==P || error(\"length(a) not equal to P\")\n  xx = @MMatrix zeros(eltype(e),P+2, P+2)\n  xy = @MVector zeros(eltype(e),P+2)\n  yy = zero(eltype(e))\n  xt = @MVector ones(eltype(e), P+2)\n  if (abs(a)&lt;1)\n    xt[3:(P+2)] .= y0\n  else\n    xt[3:(P+2)] .= 0.0\n  end\n  α = @MVector zeros(eltype(e),P+2)\n  @simd for i = 1:P\n    α[2+i] = a[i]\n  end\n\n  xx[1,1] = T-P # = 1'*1\n  xx[1,2] = xx[2,1] = (T+1)*T/2 - sum(1:P) # sum(P+1:T)\n  xx[2,2] = (2*(T)+1)*(T)*(T+1)/6 - sum((1:P).^2) # sum((P+1:T).^2)\n  @inbounds for t in (P+1):T\n    et = e[rindex()]\n    xt[2] = t\n    for i in 1:(P+2)\n      @simd for j in 3:(P+2)\n        xx[i,j] += xt[i]*xt[j]\n      end\n    end\n    y = dot(α, xt) + et\n    @simd for i in 1:(P+2)\n      xy[i] += xt[i]*y\n    end\n    yy += y^2\n    if (P&gt;1)\n      xt[4:(P+2)] .= xt[3:(P+1)]\n    end\n    xt[3] = y\n  end\n  @inbounds for i in 3:(P+2)\n    for j in 1:(i-1)\n      xx[i,j] = xx[j,i]\n    end\n  end\n  ixx = inv(xx)\n  θ = ixx*xy\n  ee = yy - xy'*ixx*xy\n  se = sqrt.(abs.(diag(ixx *(ee))./(T-(2*P+2))))\n  (θ=θ,se=se)\nend\nWe can also write SIMD instructions ourselves. The SIMD.jl package makes this somewhat accessible. For an example, resids_simd uses this package to match performance of the @turbo version.\nestimator(y0=y0,e=est.e) = function(a)\n  out = simulate_estimate_arp(y0,a,e)\n  (out.θ[3], out.se[3])\nend\nbench_sea=@benchmark  (b,t) = gridbootstrap(estimator(), a-&gt;a, αgrid, nboot)\nBenchmarkTools.Trial: 235 samples with 1 evaluation.\n Range (min … max):  20.346 ms … 39.360 ms  ┊ GC (min … max): 0.00% … 43.55\n%\n Time  (median):     20.575 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   21.340 ms ±  3.394 ms  ┊ GC (mean ± σ):  3.07% ±  8.62\n%\n\n  █▆▂▁                                                         \n  ████▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▅▄▄ ▅\n  20.3 ms      Histogram: log(frequency) by time      38.7 ms &lt;\n\n Memory estimate: 2.94 MiB, allocs estimate: 60904.\nGenerally, if @turbo successfully inserted SIMD instructions and made your code substantially faster, it will not be worth your effort to try to manually write SIMD code. However, @turbo will not always be able to insert SIMD instructions. One way to check is through benchmarking. Another way is to inspect @code_llvm ARGridBootstrap.resids_turbo!(e,y,θ). Things like fadd fast &lt;4 x double&gt; are SIMD instructions. The &lt;4 x double&gt; part is the key sign. In contrast, something like %26 = fsub double %24, %25 are scalar instructions.\nThe loops in xx_xy! are not automatically vectorized. Part of the issue is that @turbo cannot tell that xx and xy are statically allocated. If we rewrite the code to use scalars, it would like get vectorized by @turbo. The b_est_stride function does this. However, it is really inconvenient to rewrite array code as scalars. It may be more maintainable to keep the arrays and write SIMD instructions ourselves.\ne = zeros(length(y)-1)\nθ = @MVector zeros(3)\n@benchmark ARGridBootstrap.resids!($e,$y,$θ)\nBenchmarkTools.Trial: 10000 samples with 386 evaluations.\n Range (min … max):  242.459 ns … 525.808 ns  ┊ GC (min … max): 0.00% … 0.0\n0%\n Time  (median):     248.790 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   254.012 ns ±  18.579 ns  ┊ GC (mean ± σ):  0.00% ± 0.0\n0%\n\n     █▆        ▃    ▂                                           ▁\n  ▃▁███▆▄▁▁▁▃▁▇█▆▆▇███▆▄▅▆▇▄▃▆▆▇█▇▇▇▇▇▇▇▆▇▆▆▆▆▆▆▆▆▆▅▆▅▆▅▅▅▇▆▅▅▅ █\n  242 ns        Histogram: log(frequency) by time        340 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\n@benchmark ARGridBootstrap.resids_turbo!($e,$y,$θ)\nBenchmarkTools.Trial: 10000 samples with 981 evaluations.\n Range (min … max):  61.371 ns … 193.580 ns  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     62.296 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   63.836 ns ±   4.699 ns  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n   ▆█▃       ▅    ▄    ▁     ▁                                 ▁\n  ████▄▁▁▁▁▅███▃▃▆██▄▅▅██▇▇▆▆█▇▆▆▆▇▆▇▆▆▆▅▅▆▆▅▆▅▄▅▆▇▅▃▄▄▄▅▃▅▃▁▃ █\n  61.4 ns       Histogram: log(frequency) by time      86.4 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\nThis makes the code faster by a factor of more than 10. The Val(N) argument controls the width of vectors that gets passed to SIMD instructions. The value of N can affect execution by a factor of 5 or more. The best choice of N depends on your exact hardware and the code being executed.\nTo see how much this is worth it, let’s benchmark the full bootstrap code, but using the SIMD versions of resids! and xx_xy!\n@benchmark ARGridBootstrap.resids_simd!($e,$y,$θ, Val(8))\nBenchmarkTools.Trial: 10000 samples with 979 evaluations.\n Range (min … max):  64.349 ns … 119.638 ns  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     65.634 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   66.531 ns ±   3.598 ns  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n  ▁▅█▄▅▇▅▁       ▂▂ ▁▃▁  ▂   ▂                                 ▂\n  ████████▄▁▁▁▁▃▄██▇███▆▄██▅██▆▅▃▇▇▆▇█▇▇▆▆▇▆▆▇▇▆▆▆▆▆▅▆▅▅▅▅▅▆▅▆ █\n  64.3 ns       Histogram: log(frequency) by time      81.8 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\nWe have not improved total execution very much. The problem is that very little of the total time was spent in xx_xy! and resids! to begin with. We did make those functions much faster, but they were such a small portion of total execution time, that it is not noticeable. We should focus our efforts on ar1_original if we want to improve."
  },
  {
    "objectID": "gpu.html",
    "href": "gpu.html",
    "title": "Coding for Performance",
    "section": "",
    "text": "This is"
  },
  {
    "objectID": "gpu.html#array-interface",
    "href": "gpu.html#array-interface",
    "title": "Coding for Performance",
    "section": "Array interface",
    "text": "Array interface\nThe easiest way to use a GPU in Julia is through a high level array interface. ArrayFire.jl, oneAPI.jl, and CUDA.jl each offer such interfaces. We will focus on CUDA.jl in these notes. CUDA.jl relies on Nvidia’s CUDA platform, so it only works with Nvidia GPUs. Nvidia tends to dominate GPGPU, and the GPUs available on cedar.computecanada.ca and in my desktop are Nvidia.\nUsing CUDA.CuArray is simple, but has some limitations. You create arrays on the GPU using CuArray. Any array level operation on these will then be performed efficiently on the GPU. This includes broadcast functions with . and matrix multiplies.\nusing CUDA, Random, BenchmarkTools\n\n\nN = 1000\nM = 1000\n\nfunction cuarraydemo(N,M)\n  # wrapped in a  function so that the CuArrays are freed\n  # otherwise we will run out GPU memory later\n  A = randn(N,M);\n  b = randn(M,2);\n  println(\"Time on CPU\")\n  function foo(A,b)\n    (A.^2)*b\n  end\n  @time c=foo(A,b);\n  @time c=foo(A,b);\n  A_gpu = CuArray(A); # copy of A in GPU memory\n  b_gpu = CuArray(b);\n  println(\"Computations on the GPU are fast\")\n  # @btime does not work inside a function\n  @time CUDA.@sync c_gpu=foo(A_gpu,b_gpu);\n  @time CUDA.@sync c_gpu=foo(A_gpu,b_gpu);\n  println(\"But copying to and from GPU memory is not\")\n  bar(A,b) =Array(foo(CuArray(A), CuArray(b)))\n  @time c2=bar(A,b);\n  @time c2=bar(A,b);\nend\ncuarraydemo (generic function with 1 method)\njulia&gt; cuarraydemo(N,M);\nTime on CPU\n  0.017174 seconds (3 allocations: 7.645 MiB)\n  0.008491 seconds (3 allocations: 7.645 MiB)\nComputations on the GPU are fast\n  0.000287 seconds (80 allocations: 3.125 KiB)\n  0.000218 seconds (80 allocations: 3.125 KiB)\nBut copying to and from GPU memory is not\n  0.001545 seconds (102 allocations: 19.578 KiB)\n  0.001167 seconds (102 allocations: 19.578 KiB)\nCuArrays also allow indexing, so you could use loops and other constructs. However, this will not be fast. CuArrays by itself will be a good method to utilize GPUs when the code is dominated by operations on large arrays.\nUnfortunately, the fastest version of our grid bootstrap code does not fit that description. A loop seems needed to generate \\(y\\) due to the recursiveness of the AR(1) model. The fastest version of the code above involves many operations on small 3x3 arrays.\nEXERCISE: modify b_est_original or b_est_mldivide to utilize CuArrays. The approach taken in those functions involves some moderate sized matrices, so it may benefit from CuArrays."
  },
  {
    "objectID": "gpu.html#custom-cuda-kernels",
    "href": "gpu.html#custom-cuda-kernels",
    "title": "Coding for Performance",
    "section": "Custom CUDA Kernels",
    "text": "Custom CUDA Kernels\nTo parallelize the code above on a GPU, we will have to use a lower level interface to the GPU. To explain how it works, we will begin with a simple example that just squares all the elements of an array.\nDisclaimer: my understanding of CUDA and the inner workings of GPUs is far from complete. Some of the details in this section might be inaccurate.\nA typical workflow with CUDA consists of\n\nAllocate GPU memory and copying arrays into it with CuArray.\nDecide how many threads and what configuration of threads to launch.\nEach thread does some computation by running a “kernel” function.\nCopy result from GPU memory to CPU memory.\n\nIn the code below, 1 happens in cuarray_cudanative_compare, 2 happens in the square! function, square_kernel! is the kernel in 3, and 4 is just not done.\n\nThreads and blocks\nCUDA organizes GPU threads into blocks. I believe that the threads in a block all execute concurrently. Threads in the same block share some memory and registers. All current Nvidia GPUs have a maximum number of threads per block of 1024. Note that threads in the same block share registers1, and different kernel functions will use different numbers of registers at once, so depending on the kernel function, you might be limited to fewer than 1024 threads per block. The number of registers available per block depends on your GPU. You can check your GPU characteristics by compiling and running the C++ program in $CUDA_PATH/samples/1_Utilities/deviceQuery/. Alternatively, you can see this information in Julia by running the code below.\nprintln(\"Maximum threads per block $(attribute(device(), CUDA.CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK))\")\nprintln(\"Maximum x blocks $(attribute(device(), CUDA.CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X))\")\nprintln(\"Maximum registers per block $(attribute(device(), CUDA.CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK))\")\nMaximum threads per block 1024\nMaximum x blocks 2147483647\nMaximum registers per block 65536\nThere is no simple way to predict how many registers a kernel function uses. It will depend both on the code you write and how the compiler optimizes the code. If you encounter cryptic error messages about CUDA resources unavailable, then try reducing the number of threads per block. Alternatively, you can limit the number of registers used by passing the maxregs argument to @cuda.\nYou can execute more than 1024 threads by specifying a number of blocks. There is also a limit to the number of blocks, but it is rather large. In the code below, we set the number of blocks, so that nblocks*nthreads &gt;= length(A). Each thread then operates on a single element of A. When the code is executed, each thread has a unique threadIdx and blockIdx combination, and these are used to assign threads to elements of A. The indices go from 1 to number of threads (or blocks). For convenience you can request threads and blocks to have up 3 dimensions, and there are threadIdx().y and threadIdx().z for the additional dimensions.\nfunction square!(A::CuArray)\n  n = length(A)\n  maxthreads = 1024\n  nthreads = min(maxthreads, n)\n  nblocks  = Int(ceil(n/nthreads))\n\n  @cuda threads=nthreads blocks=nblocks square_kernel!(A)\n\n  return A\nend\n\nfunction square_kernel!(A)\n  i = threadIdx().x + (blockIdx().x-1)*blockDim().x\n  if (i&lt;=length(A))\n    @inbounds A[i] *= A[i]\n  end\n  return nothing # CUDA kernels must return nothing\nend\n\nfunction cuarray_cudanative_compare(A)\n  A_gpu = CuArray(A);\n  println(\"CUDAnative square!\")\n  @time CUDA.@sync square!(A_gpu);\n  @time CUDA.@sync square!(A_gpu);\n\n  println(\"CuArray A*=A\")\n  A_gpu = CuArray(A);\n  @time CUDA.@sync A_gpu .*= A_gpu;\n  @time CUDA.@sync A_gpu .*= A_gpu;\n  return nothing\nend\ncuarray_cudanative_compare (generic function with 1 method)\njulia&gt; cuarray_cudanative_compare(randn(N,M))\nCUDAnative square!\n  0.000194 seconds (8 allocations: 400 bytes)\n  0.000129 seconds (8 allocations: 400 bytes)\nCuArray A*=A\n  0.000201 seconds (31 allocations: 2.109 KiB)\n  0.000135 seconds (31 allocations: 2.109 KiB)\n\n\nKernel Limitations\nCUDA kernel functions execute on the GPU and in GPU memory. Since GPU memory is allocated and managed differently than RAM, many Julia functions will not work in CUDA kernels. Most importantly, Julia functions that allocate dynamically sized arrays will not work. This means that even matrix multiplication like θ = ixx*xy will fail (if ixx or xy are dynamically allocated) since it allocates an array for θ. You can, however, have local scalars, tuples, and StaticArrays within a kernel function. The key difference is that the sizes of these types are known at compile time. If ixx and xy are StaticArrays, then you can do something like θ = ixx*xy. Since the compiler knows the size of ixx and xy, the compiler also know the size of θ. However, even with StaticArrays you must be careful with operations that that create new StaticArrays (like matrix multiplies). These will cause problems if called repeatedly within a loop.2\nIt is possible to dynamicaaly allocate GPU memory within a kernel function, but it requires using the low-level interface to CUDA in CUDA.jl. Moreoever, it is generally not a good idea to be dynamically allocating and freeing memory in each of the thousands of threads you execute.3"
  },
  {
    "objectID": "gpu.html#footnotes",
    "href": "gpu.html#footnotes",
    "title": "Coding for Performance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProcessor registers are the fastest bits of memory on the processor, and registers are where the actual addition, multiplication, and other instructions are carried out.↩︎\nIf you create StaticArrays inside a loop, they get allocated to the GPU’s “dynamic shared memory.” I believe a new allocation happens each loop iteration. This will be slow, and there is a fairly small amount of dynamic shared memory, of which you will soon run out.↩︎\nThere are situations where allocating shared memory is needed and a good idea, but these require some advanced techniques that we will not cover.↩︎"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "ARGridBootstrap",
    "section": "",
    "text": "The notes and examples are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf.\n\n\n\n\n\nBibTeX citation.\nThe license for the package source code is here."
  }
]