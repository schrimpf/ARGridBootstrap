[
  {
    "objectID": "threads.html",
    "href": "threads.html",
    "title": "Coding for Performance",
    "section": "",
    "text": "Current computers almost all have multiple cores. We can divide the time it takes our code by up to the number of cores we have (but usually less) by writing multi-threaded code. Multi-threaded code performs multiple tasks at once with shared memory. Before you begin writing multi-threaded code, you should make sure your code isn’t already using all available cores. It is likely that the BLAS and Lapack libraries that Julia uses for linear algebra are multi-threaded. If you code is dominated by large matrix operations, it may already be using all available cores. In that case, there will not be much benefit from additional multi-threading.\nRead “The Basics of Single Node Parallel Computing” Rackauckus (2019) (Rackauckas 2019) .\nOnce we have decided that the code might benefit from multi-threading, we should look for loops (or other independent tasks) that can be multi-threaded. There is some overhead from creating threads and communicating among them. Multi-threading generally works best for loops where each iteration involves substantial work, and each iteration is independent of all others. The loops over grid points and bootstrap repetitions in gridbootstrap are perfect candidates. We don’t care about the order in which these loops get executed. The result of each iteration is (mostly) independent of all others.\nusing ARGridBootstrap, CodeTracking, Random, BenchmarkTools, Profile, ProfileCanvas\nfunction code_md(s)\n  println(\"```julia\\n\"*s*\"\\n```\\n\")\nend\nT = 200\ne = randn(T)\ny0 = 0\na = 0.9\ny = ar1_original(y0, a, e)\nest = b_est_original(y)\nαgrid = 0.84:(0.22/50):1.06\nnboot= 199\nwrapper(b_est) = function(x)\n  out=b_est(x)\n  (out.θ[3], out.se[3])\nend\n\n\ns=@code_string gridbootstrap_threaded(wrapper(b_est_original), (a, rng)-&gt;ar1_original(y0, a, est.e, n-&gt;rand(rng,1:(T-1),n)),αgrid, 2)\ncode_md(s)\nfunction gridbootstrap_threaded(estimator, simulator,\n                                grid::AbstractVector,\n                                nboot=199)\n  g = length(grid)\n  bootq = zeros(nboot, g)\n  ba    = zeros(nboot, g)\n  bootse = zeros(nboot,g)\n  #@threads for ak in 1:g\n  #  for j in 1:nboot\n  @threads for ind ∈ CartesianIndices(ba)\n    j = ind[1]\n    ak = ind[2]\n    (bootq[j,ak], bootse[j,ak]) = estimator(simulator(grid[ak],Random.TaskLocalRNG()))\n    ba[j,ak] = bootq[j,ak] - grid[ak]\n  end\n  ts = ba./bootse\n  (ba=ba, t=ts)\nend\n\n\n\nNow, let’s try multi-threading the original version of the code.\njulia&gt; using Base.Threads\n\njulia&gt; println(\"Single thread, original version\")\nSingle thread, original version\n\njulia&gt; @benchmark begin \n         (b,t) = gridbootstrap(wrapper(b_est_original), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 199)\n       end\nBenchmarkTools.Trial: 27 samples with 1 evaluation.\n Range (min … max):  175.276 ms … 199.619 ms  ┊ GC (min … max): 11.56% … 10.62%\n Time  (median):     186.845 ms               ┊ GC (median):    16.06%\n Time  (mean ± σ):   186.054 ms ±   4.257 ms  ┊ GC (mean ± σ):  15.72% ±  1.64%\n\n  ▁                    ▁ ▁  ▁▄ ▄▄▁█                              \n  █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁█▆█▁▁██▆████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆ ▁\n  175 ms           Histogram: frequency by time          200 ms &lt;\n\n Memory estimate: 219.44 MiB, allocs estimate: 345075.\njulia&gt; println(\"$(Threads.nthreads()) threads, original version\")\n40 threads, original version\n\njulia&gt; @benchmark begin \n         (b,t) = gridbootstrap_threaded(wrapper(b_est_original),\n                                        (a, rng)-&gt;ar1_original(y0, a, est.e, n-&gt;rand(rng,1:(T-1),n)),\n                                        αgrid, 199)\n       end\nBenchmarkTools.Trial: 32 samples with 1 evaluation.\n Range (min … max):  102.512 ms … 661.604 ms  ┊ GC (min … max):  0.00% … 81.64%\n Time  (median):     104.351 ms               ┊ GC (median):     0.00%\n Time  (mean ± σ):   158.058 ms ± 161.796 ms  ┊ GC (mean ± σ):  31.58% ± 24.16%\n\n  █                                                              \n  █▄▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▃ ▁\n  103 ms           Histogram: frequency by time          662 ms &lt;\n\n Memory estimate: 311.76 MiB, allocs estimate: 6383971.\nThe execution times are nearly identical on my computer. The reason is that the computation is dominated by the creation of X and multiplying X'*X and X'*y. These operations are already multi-threaded in the BLAS library being used. It is possible that first calling using LinearAlgebra; BLAS.set_num_threads(1) would improve the performance of the multi-threaded bootstrap.\n\n\n\njulia&gt; println(\"Single thread, fastest version\")\nSingle thread, fastest version\n\njulia&gt; estimator(y0=y0,e=est.e) = function(a)\n         out = simulate_estimate_arp(y0,a,e)\n         (out.θ[3], out.se[3])\n       end\nestimator (generic function with 3 methods)\n\njulia&gt; @benchmark  (b,t) = gridbootstrap(estimator(), a-&gt;a, αgrid, nboot)\nBenchmarkTools.Trial: 242 samples with 1 evaluation.\n Range (min … max):  18.752 ms … 48.284 ms  ┊ GC (min … max): 0.00% … 58.59%\n Time  (median):     19.663 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   20.719 ms ±  4.339 ms  ┊ GC (mean ± σ):  3.86% ±  9.76%\n\n  ▆█▇▆▃▁                                                       \n  ██████▇▆▄▁▁▁▁▁▆▄▄▁▁▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▄▁▁▁▁▁▁▁▁▇ ▆\n  18.8 ms      Histogram: log(frequency) by time      42.7 ms &lt;\n\n Memory estimate: 2.94 MiB, allocs estimate: 60904.\njulia&gt; println(\"$(Threads.nthreads()) threads, fastest version\")\n40 threads, fastest version\n\njulia&gt; estimator_threaded(y0=y0,e=est.e)=function(foo)\n         (a, rng) = foo\n         out=simulate_estimate_arp(y0,a,e,Val(1),()-&gt;rand(rng,1:length(e)))\n         (out.θ[3], out.se[3])\n       end\nestimator_threaded (generic function with 3 methods)\n\njulia&gt; @benchmark (bs, ts) = gridbootstrap_threaded(estimator_threaded(),(a,rng)-&gt;(a,rng), αgrid,nboot)\nBenchmarkTools.Trial: 1507 samples with 1 evaluation.\n Range (min … max):  1.620 ms … 141.653 ms  ┊ GC (min … max):  0.00% … 97.00%\n Time  (median):     2.524 ms               ┊ GC (median):     0.00%\n Time  (mean ± σ):   3.301 ms ±   8.923 ms  ┊ GC (mean ± σ):  18.36% ±  6.66%\n\n         ██▅▇▃                                                 \n  ▄▅▅▄▇▆▆█████████▇▅▅▄▅▃▄▃▃▂▂▂▂▃▂▃▂▂▃▂▂▂▂▁▂▂▁▂▂▂▁▂▁▂▂▁▂▂▂▁▂▂▂ ▃\n  1.62 ms         Histogram: frequency by time        6.71 ms &lt;\n\n Memory estimate: 2.97 MiB, allocs estimate: 61138.\nNotice how the speedup from using multiple threads is far less than number of cores. On my computer, the threaded version of the code is about 7 times faster, even though my computer has 40 “cores” (or 20 physical cores. My computer has 2 processors with 10 cores each, and each core is hyperthreaded into 2. The OS sees 40 processors, but half of them are sharing substantial resources). A speedup far less than the number of cores is typical. Creating and managing multiple threads creates some overhead. Moreover, cores must share various resources; most notably RAM and some cache."
  },
  {
    "objectID": "threads.html#multi-threaded-grid-bootstrap",
    "href": "threads.html#multi-threaded-grid-bootstrap",
    "title": "Coding for Performance",
    "section": "",
    "text": "s=@code_string gridbootstrap_threaded(wrapper(b_est_original), (a, rng)-&gt;ar1_original(y0, a, est.e, n-&gt;rand(rng,1:(T-1),n)),αgrid, 2)\ncode_md(s)\nfunction gridbootstrap_threaded(estimator, simulator,\n                                grid::AbstractVector,\n                                nboot=199)\n  g = length(grid)\n  bootq = zeros(nboot, g)\n  ba    = zeros(nboot, g)\n  bootse = zeros(nboot,g)\n  #@threads for ak in 1:g\n  #  for j in 1:nboot\n  @threads for ind ∈ CartesianIndices(ba)\n    j = ind[1]\n    ak = ind[2]\n    (bootq[j,ak], bootse[j,ak]) = estimator(simulator(grid[ak],Random.TaskLocalRNG()))\n    ba[j,ak] = bootq[j,ak] - grid[ak]\n  end\n  ts = ba./bootse\n  (ba=ba, t=ts)\nend"
  },
  {
    "objectID": "threads.html#some-libraries-are-already-multi-threaded",
    "href": "threads.html#some-libraries-are-already-multi-threaded",
    "title": "Coding for Performance",
    "section": "",
    "text": "Now, let’s try multi-threading the original version of the code.\njulia&gt; using Base.Threads\n\njulia&gt; println(\"Single thread, original version\")\nSingle thread, original version\n\njulia&gt; @benchmark begin \n         (b,t) = gridbootstrap(wrapper(b_est_original), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 199)\n       end\nBenchmarkTools.Trial: 27 samples with 1 evaluation.\n Range (min … max):  175.276 ms … 199.619 ms  ┊ GC (min … max): 11.56% … 10.62%\n Time  (median):     186.845 ms               ┊ GC (median):    16.06%\n Time  (mean ± σ):   186.054 ms ±   4.257 ms  ┊ GC (mean ± σ):  15.72% ±  1.64%\n\n  ▁                    ▁ ▁  ▁▄ ▄▄▁█                              \n  █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁█▆█▁▁██▆████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆ ▁\n  175 ms           Histogram: frequency by time          200 ms &lt;\n\n Memory estimate: 219.44 MiB, allocs estimate: 345075.\njulia&gt; println(\"$(Threads.nthreads()) threads, original version\")\n40 threads, original version\n\njulia&gt; @benchmark begin \n         (b,t) = gridbootstrap_threaded(wrapper(b_est_original),\n                                        (a, rng)-&gt;ar1_original(y0, a, est.e, n-&gt;rand(rng,1:(T-1),n)),\n                                        αgrid, 199)\n       end\nBenchmarkTools.Trial: 32 samples with 1 evaluation.\n Range (min … max):  102.512 ms … 661.604 ms  ┊ GC (min … max):  0.00% … 81.64%\n Time  (median):     104.351 ms               ┊ GC (median):     0.00%\n Time  (mean ± σ):   158.058 ms ± 161.796 ms  ┊ GC (mean ± σ):  31.58% ± 24.16%\n\n  █                                                              \n  █▄▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▃ ▁\n  103 ms           Histogram: frequency by time          662 ms &lt;\n\n Memory estimate: 311.76 MiB, allocs estimate: 6383971.\nThe execution times are nearly identical on my computer. The reason is that the computation is dominated by the creation of X and multiplying X'*X and X'*y. These operations are already multi-threaded in the BLAS library being used. It is possible that first calling using LinearAlgebra; BLAS.set_num_threads(1) would improve the performance of the multi-threaded bootstrap."
  },
  {
    "objectID": "threads.html#multi-threading-where-it-matters",
    "href": "threads.html#multi-threading-where-it-matters",
    "title": "Coding for Performance",
    "section": "",
    "text": "julia&gt; println(\"Single thread, fastest version\")\nSingle thread, fastest version\n\njulia&gt; estimator(y0=y0,e=est.e) = function(a)\n         out = simulate_estimate_arp(y0,a,e)\n         (out.θ[3], out.se[3])\n       end\nestimator (generic function with 3 methods)\n\njulia&gt; @benchmark  (b,t) = gridbootstrap(estimator(), a-&gt;a, αgrid, nboot)\nBenchmarkTools.Trial: 242 samples with 1 evaluation.\n Range (min … max):  18.752 ms … 48.284 ms  ┊ GC (min … max): 0.00% … 58.59%\n Time  (median):     19.663 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   20.719 ms ±  4.339 ms  ┊ GC (mean ± σ):  3.86% ±  9.76%\n\n  ▆█▇▆▃▁                                                       \n  ██████▇▆▄▁▁▁▁▁▆▄▄▁▁▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▄▁▁▁▁▁▁▁▁▇ ▆\n  18.8 ms      Histogram: log(frequency) by time      42.7 ms &lt;\n\n Memory estimate: 2.94 MiB, allocs estimate: 60904.\njulia&gt; println(\"$(Threads.nthreads()) threads, fastest version\")\n40 threads, fastest version\n\njulia&gt; estimator_threaded(y0=y0,e=est.e)=function(foo)\n         (a, rng) = foo\n         out=simulate_estimate_arp(y0,a,e,Val(1),()-&gt;rand(rng,1:length(e)))\n         (out.θ[3], out.se[3])\n       end\nestimator_threaded (generic function with 3 methods)\n\njulia&gt; @benchmark (bs, ts) = gridbootstrap_threaded(estimator_threaded(),(a,rng)-&gt;(a,rng), αgrid,nboot)\nBenchmarkTools.Trial: 1507 samples with 1 evaluation.\n Range (min … max):  1.620 ms … 141.653 ms  ┊ GC (min … max):  0.00% … 97.00%\n Time  (median):     2.524 ms               ┊ GC (median):     0.00%\n Time  (mean ± σ):   3.301 ms ±   8.923 ms  ┊ GC (mean ± σ):  18.36% ±  6.66%\n\n         ██▅▇▃                                                 \n  ▄▅▅▄▇▆▆█████████▇▅▅▄▅▃▄▃▃▂▂▂▂▃▂▃▂▂▃▂▂▂▂▁▂▂▁▂▂▂▁▂▁▂▂▁▂▂▂▁▂▂▂ ▃\n  1.62 ms         Histogram: frequency by time        6.71 ms &lt;\n\n Memory estimate: 2.97 MiB, allocs estimate: 61138.\nNotice how the speedup from using multiple threads is far less than number of cores. On my computer, the threaded version of the code is about 7 times faster, even though my computer has 40 “cores” (or 20 physical cores. My computer has 2 processors with 10 cores each, and each core is hyperthreaded into 2. The OS sees 40 processors, but half of them are sharing substantial resources). A speedup far less than the number of cores is typical. Creating and managing multiple threads creates some overhead. Moreover, cores must share various resources; most notably RAM and some cache."
  },
  {
    "objectID": "index.html#functions",
    "href": "index.html#functions",
    "title": "ARGridBootstrap.jl",
    "section": "Functions",
    "text": "Functions\n\nARGridBootstrap.ar1_original\nARGridBootstrap.argridbootstrap_gpu\nARGridBootstrap.argridkernel!\nARGridBootstrap.b_est_mldivide\nARGridBootstrap.b_est_nox\nARGridBootstrap.b_est_original\nARGridBootstrap.b_est_stride\nARGridBootstrap.gridbootstrap\nARGridBootstrap.gridbootstrap_threaded\nARGridBootstrap.invsym\nARGridBootstrap.oneto\nARGridBootstrap.rngarray\nARGridBootstrap.simulate_estimate_arp\nARGridBootstrap.simulate_estimate_arp_lv\n\n\n\n\nAR\n# ARGridBootstrap.ar1_original — Function.\nar1_original(y0, a, e, rindex=T-&gt;rand(1:length(e), T))\nSimulate AR1 model by sampling errors from e with replacement.\ny[t] = a*y[t-1] + ϵ[t]\nArguments\n\ny0: initial value for y\na: AR parameter\ne: values of for error term. ϵ = e[rindex(T)]]\nrindex function that returns random index in 1:length(e)\n\nReturns\n\ny: vector of length T = length(e)\n\nsource\n# ARGridBootstrap.b_est_mldivide — Method.\nb_est_mldivide(y)\nEstimate AR(1) model with intercept and time trend.\ny[t] = θ[0] + θ[1]t + θ[2]y[t-1] + e[t]\nArguments\n\ny: vector\n\nReturns\n\nθ: estimated coefficients\nse: standard errors\ne: residuals\n\nsource\n# ARGridBootstrap.b_est_nox — Method.\nb_est_nox(y)\nEstimate AR(1) model with intercept and time trend.\ny[t] = θ[0] + θ[1]t + θ[2]y[t-1] + e[t]\nArguments\n\ny: vector\n\nReturns\n\nθ: estimated coefficients\nse: standard errors\ne: residualas\n\nsource\n# ARGridBootstrap.b_est_original — Method.\nb_est_original(y)\nEstimate AR(1) model with intercept and time trend\ny[t] = θ[0] + θ[1]t + θ[2]y[t-1] + e[t]\nArguments\n\ny: vector\n\nReturns\n\nθ: estimated coefficients\nse: standard errors\ne: residuals\n\nsource\n# ARGridBootstrap.b_est_stride — Method.\nb_est_stride(y)\nEstimate AR(1) model with intercept and time trend.\ny[t] = θ[0] + θ[1]t + θ[2]y[t-1] + e[t]\nArguments\n\ny: vector\n\nReturns\n\nθ: estimated coefficients\nse: standard errors\ne: residualas\n\nsource\n# ARGridBootstrap.invsym — Method.\nfast inverse function for statically sized 3x3 StrideArray\nsource\n# ARGridBootstrap.oneto — Method.\noneto(Val(N)) Creates at a compile time the tuple (1, 2, …, N)\nsource\n# ARGridBootstrap.simulate_estimate_arp — Method.\nsimulate_estimate_arp(y0, a, e, ar::Val{P}, rindex=T-&gt;rand(1:length(e),T))\nSimulates and estimates an AR(P) model. y is simulated as\ny[t] = a*y[t-1] + ϵ[t]\nand the estimate of θ from\ny[t] = θ[1] + θ[2]t + θ[3] y[t-1] + … + θ[P] y[t-P] + u[t]\nis computed.\nArguments\n\ny0 initial value of y\na AR(1) parameter\ne error terms to sample from ϵ[t] = e[rindex(1)]\nar::Val{P} order of autoregressive model to estimate\nrindex function that returns random index in 1:length(e)\n\nReturns\n\nθ estimated coefficients\nse standard errors\n\nsource\n# ARGridBootstrap.simulate_estimate_arp_lv — Method.\nsimulate_estimate_arp_lv(y0, a, e, ar::Val{P}, rindex=T-&gt;rand(1:length(e),T))\nSimulates and estimates an AR(P) model. Uses LoopVectorization.jl to produce fast code.\ny is simulated as\ny[t] = a*y[t-1] + ϵ[t]\nand the estimate of θ from\ny[t] = θ[1] + θ[2]t + θ[3] y[t-1] + … + θ[P] y[t-P] + u[t]\nis computed.\nArguments\n\ny0 initial value of y\na AR(1) parameter\ne error terms to sample from ϵ[t] = e[rindex(1)]\nar::Val{P} order of autoregressive model to estimate\nrindex function that returns random index in 1:length(e)\n\nReturns\n\nθ estimated coefficients\nse standard errors\n\nsource\n\n\n\n\nGrid Bootstrap\n# ARGridBootstrap.argridbootstrap_gpu — Method.\nargridbootstrap_gpu(e; αgrid = 0.84:(0.22/20):1.06,\n                      nboot=199, RealType = Float32)\nComputes grid bootstrap estimates for an AR(1) model.\nFor each α ∈ grid, repeatedly simulate data with parameter α and then compute an estimate.\nArguments\n\ne vector error terms that will be resampled with replacement to generate bootstrap sample\ngrid grid of parameter values. For each value, nboot datasets will be simulated and estimates computed.\nnboot\nRealType type of numbers for GPU computation. On many GPUs, Float32 will have better performance than Float64.\n\nReturns\n\nba hatα - α for each grid value and simulated dataset\nt t-stat for each grid value and simulated dataset\n\nsource\n# ARGridBootstrap.argridkernel! — Method.\nargridkernel!(ba,bootq, bootse, ar::Val{P}, e, ei, αgrid)\nGPU kernel for simulation and estimation of AR(P) model.\nArguments (modified on return)\n\nba: nboot × ngrid array. Will be filled with bootstrap estimates of α grid values of true α\nbootq: nboot × ngrid array. Will be filled with bootstrap estimates of α\nbootse: nboot × ngrid array. Will be filled with standard errors of α for each bootstrap repetition\n\nArguments (not modified)\n\nar::Val{P} : autoregressive order for estimation. Simulated model will always be AR(1) with 0 intercept and time trend, but estimation will use an AR(P) model with intercept and time trend. Only the AR(1) parameter estimate is included in ba, bootq, and bootse.\ne : error terms to draw with replacement\nei : nboot × ngrid × length(e) array of indices of e to use to generate bootstrap sample1\nαgrid : length ngrid values of AR(1) parameter to perform bootstrap on.\n\nReturns nothing, but modifies in place ba, bootq, and bootse\nsource\n# ARGridBootstrap.gridbootstrap — Function.\ngridbootstrap(estimator, simulator,\n              grid::AbstractVector,\n              nboot=199)\nComputes grid bootstrap estimates a single parameter model.\nFor each α ∈ grid, repeatedly simulate data with parameter α and then compute an estimate.\nArguments\n\nestimator function of output of simulator that returns a 2-tuple containing an estimate of α and its standard error.\nsimulator function that given α simulates data that can be used to estimate α\ngrid grid of parameter values. For each value, nboot datasets will be simulated and estimates computed.\nnboot\n\nReturns\n\nba hatα - α for each grid value and simulated dataset\nt t-stat for each grid value and simulated dataset\n\nsource\n# ARGridBootstrap.gridbootstrap_threaded — Function.\ngridbootstrap_threaded(estimator, simulator,\n                grid::AbstractVector,\n                nboot=199, rng=rngarray(nthreads())\nComputes grid bootstrap estimates a single parameter model.\nMultithreaded version.\nFor each α ∈ grid, repeatedly simulate data with parameter α and then compute an estimate.\nArguments\n\nestimator function of output of simulator that returns a 2-tuple containing an estimate of α and its standard error.\nsimulator function that given α and rng, simulates data that can be used to estimate α\ngrid grid of parameter values. For each value, nboot datasets will be simulated and estimates computed.\nnboot number of bootstrap simulations per grid point\n\nReturns\n\nba hatα - α for each grid value and simulated dataset\nt t-stat for each grid value and simulated dataset\n\nsource\n# ARGridBootstrap.rngarray — Method.\nrngarray(n)\nCreate n rng states that will not overlap for 10^20 steps.\nNote: this will be unneeded in Julia 1.3 when thread-safe RNG is included.\nsource"
  },
  {
    "objectID": "assignment.html",
    "href": "assignment.html",
    "title": "Coding for Performance",
    "section": "",
    "text": "For the assignment, most students focused on optimizing either the share function from our earlier BLP example, or the statparts function from GMMInference.\nLet’s see how we can improve these functions’ performance."
  },
  {
    "objectID": "assignment.html#initial-benchmark",
    "href": "assignment.html#initial-benchmark",
    "title": "Coding for Performance",
    "section": "Initial Benchmark",
    "text": "Initial Benchmark\nJ = 10\nK = 5\nδ = rand(J)\nX = randn(J,K)\nΣ = I + zeros(K,K)\n∫ = Integrator(MvNormal(zeros(K),I));\n@benchmark share(δ,Σ,X,∫)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  46.078 μs …  10.655 ms  ┊ GC (min … max): 0.00% … 98.7\n0%\n Time  (median):     55.581 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   67.335 μs ± 199.461 μs  ┊ GC (mean ± σ):  8.98% ±  3.3\n8%\n\n     ▁▁    ▂▃▅▇█▇▄▄▄▂▁▁                    ▁▂▂  ▂▂▄▄▃▃▄▂▁▁     ▂\n  ▆▇████████████████████▇▆▅▇▇▆▆▆▅▆▆▆▄▅▃▄▄▁▇█████████████████▇▆ █\n  46.1 μs       Histogram: log(frequency) by time      87.2 μs &lt;\n\n Memory estimate: 65.48 KiB, allocs estimate: 499.\nProfiling it:\nusing Profile\nProfile.clear();\nProfile.init(n=10^7,delay=0.00001);\n@profile sum(share(δ,Σ,X,∫) for i ∈ 1:1000)\nprofilehtmlstring()\n\n\n\n\nIt’s a good idea to check @code_warntype and verify that there are no type instabilities (variables with type Any or Union). I checked and there are no type instabilites here."
  },
  {
    "objectID": "assignment.html#loopvectorization",
    "href": "assignment.html#loopvectorization",
    "title": "Coding for Performance",
    "section": "LoopVectorization",
    "text": "LoopVectorization\nExamining the profile, we see that the multiplication of x*Σ*ν is the single most costly operation. The second most costly are lines computing the softmax function, exp.(s)./(1 .+exp.(s))\nusing LoopVectorization\n\n@inline function dplusxMy(d::AbstractVector,x::AbstractMatrix,M::AbstractMatrix,y::AbstractVector)\n  out = similar(d)\n  @turbo for i ∈ axes(x,1)\n    r = d[i]\n    for j ∈ axes(x,2)\n      for k ∈ axes(M,2)\n        r += x[i,j]*M[j,k]*y[k]\n      end\n    end\n    out[i] = r\n  end\n  out\nend\n@inline function softmax0!(s)\n  smax=@turbo reduce(max, s, init=zero(eltype(s)))\n  s .= exp.(s .- smax)\n  s ./= (sum(s) + exp(zero(smax)-smax))\n  return(s)\nend\nfunction share_v2(δ, Σ, x, ∫)\n  J,K = size(x)\n  (length(δ) == J) || error(\"length(δ)=$(length(δ)) != size(x,1)=$J\")\n  (K,K) == size(Σ) || error(\"size(x,2)=$K != size(Σ)=$(size(Σ))\")\n  function shareν(ν)\n    s = dplusxMy(δ,x,Σ,ν)\n    softmax0!(s)\n    s\n  end\n  return(∫(shareν))\nend\n@benchmark share_v2(δ,Σ,X,∫)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  28.545 μs …   4.955 ms  ┊ GC (min … max): 0.00% … 98.4\n1%\n Time  (median):     33.795 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   41.119 μs ± 117.942 μs  ┊ GC (mean ± σ):  6.94% ±  2.4\n1%\n\n             ▆█▂                                                \n  ▂▂▂▂▂▂▂▂▂▃▇███▄▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂▂▂▃▃▂▄▄▃▃▃▄▅▃ ▃\n  28.5 μs         Histogram: frequency by time         53.3 μs &lt;\n\n Memory estimate: 42.05 KiB, allocs estimate: 299.\nBy writing the δ+x*Σ*ν as loop we avoid some allocations, and can use @turbo to insert SIMD instructions. This gives a noticeable speedup. About half the gains here are from writing out the loops in dplusxMy and half from using @turbo. The changes to the softmax calculation did not make much difference.\nusing Profile\nProfile.clear();\nProfile.init(n=10^7,delay=0.00001);\nProfileCanvas.@profview sum(share_v2(δ,Σ,X,∫) for i ∈ 1:1000)\nprofilehtmlstring()\n\n\n\n\nThe execution time is still dominated by the two functions mentioned above."
  },
  {
    "objectID": "assignment.html#staticarrays",
    "href": "assignment.html#staticarrays",
    "title": "Coding for Performance",
    "section": "StaticArrays",
    "text": "StaticArrays\nSince the calculations involve small arrays, it is likely to benefit from using StaticArrays.\nusing StaticArrays\nsδ = SVector{J}(δ)\nsΣ = SMatrix{K,K}(Σ)\nsX = SMatrix{J,K}(X)\nnd = length(∫.x)\niw = SVector{nd}(fill(1/nd,nd))\nix = [SVector{K}(x) for x ∈ ∫.x]\ns∫ = Integrator(ix,iw)\n@benchmark share_v2(sδ,sΣ,sX,s∫)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  18.543 μs …   5.089 ms  ┊ GC (min … max): 0.00% … 98.5\n7%\n Time  (median):     23.061 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   28.211 μs ± 110.996 μs  ┊ GC (mean ± σ):  8.71% ±  2.2\n0%\n\n               ▄█▆▂                                             \n  ▂▃▃▃▃▃▃▂▂▂▃▄▇████▆▃▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▂▂▂▃▇▆▄▄▇▅▃▂▂▂ ▃\n  18.5 μs         Histogram: frequency by time         36.1 μs &lt;\n\n Memory estimate: 28.03 KiB, allocs estimate: 299.\nThe code is now 2 times faster than what we started with.\nInterestly, slightly better performance can be achieved by simply passing StaticArrays to the original code.\n@benchmark share($(MVector(sδ)),sΣ,sX,s∫) # δ gets mutated, so we must make it a Mutable SArray.\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  15.258 μs …   5.028 ms  ┊ GC (min … max): 0.00% … 98.7\n4%\n Time  (median):     18.066 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   25.243 μs ± 111.254 μs  ┊ GC (mean ± σ):  9.76% ±  2.2\n1%\n\n         ▇█▁                                                    \n  ▂▃▃▃▃▂▃███▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▂▁▁▂▁▁▁▂▂▃▅▅▆▆▅▅▆▄▂▂▂▂▂ ▃\n  15.3 μs         Histogram: frequency by time         33.2 μs &lt;\n\n Memory estimate: 28.03 KiB, allocs estimate: 299."
  },
  {
    "objectID": "assignment.html#non-allocating",
    "href": "assignment.html#non-allocating",
    "title": "Coding for Performance",
    "section": "Non-allocating",
    "text": "Non-allocating\nWe can do still better by eliminating the allocations. With normal arrays, broadcast operations are usually non-allocating, and normal array operations allocate. With StaticArrays, normal operatorations do not allocate.1\nAdditionally, we can precompute x*Σ outside of shareν (we could have done this at any time earlier too). This change accounts for about 1μs of the speed up.\nfunction share_v3(δ, Σ, x, ∫)\n  J,K = size(x)\n  (length(δ) == J) || error(\"length(δ)=$(length(δ)) != size(x,1)=$J\")\n  (K,K) == size(Σ) || error(\"size(x,2)=$K != size(Σ)=$(size(Σ))\")\n  xΣ = x*Σ\n  function shareν(ν)\n    s = δ + xΣ*ν\n    smax=max(0,maximum(s))\n    s -= smax*ones(typeof(s))\n    s = exp.(s)\n    s *= 1/(sum(s) + exp(0-smax))\n    return(s)\n  end\n  return(∫(shareν))\nend\nshare_v3 (generic function with 1 method)\n@benchmark share_v3(sδ,sΣ,sX,s∫)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  9.609 μs …  24.355 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     9.706 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   9.743 μs ± 337.037 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n         ▁▂▃█▁▂▅                                               \n  ▁▁▁▁▂▄▄███████▇▆▇▄▅▆▅▇█▅▆▆▄▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▃\n  9.61 μs         Histogram: frequency by time          10 μs &lt;\n\n Memory estimate: 96 bytes, allocs estimate: 1.\nThe code is now 5 times faster than the original, and only allocates once (to store the return value)."
  },
  {
    "objectID": "assignment.html#multi-threading",
    "href": "assignment.html#multi-threading",
    "title": "Coding for Performance",
    "section": "Multi-Threading",
    "text": "Multi-Threading\nMulti-threading this code is difficult because it is already quite fast. The sum in the integral can be parallelized, but unless there are a large number of integration points, the overhead from creating threads will likely outweigh the benefits.\nThe Polyester package provides a faster, but more limited threading model than base Julia. Using it, we can see modest gains, even with just 100 integration points.\nTo get type stability, I had to write the integration sum loop inside the share function. It would have been cleaner to keep the integration sum in a separate function, but I could not make it type stable that way.\nimport Polyester\nfunction share_v4(δ, Σ, x, ∫, ::Val{B}=Val(length(∫.x) ÷ 10)) where {B}\n  J,K = size(x)\n  (length(δ) == J) || error(\"length(δ)=$(length(δ)) != size(x,1)=$J\")\n  (K,K) == size(Σ) || error(\"size(x,2)=$K != size(Σ)=$(size(Σ))\")\n  xΣ = x*Σ\n  function shareν(ν)\n    s = δ + xΣ*ν\n    smax=max(0,maximum(s))\n    s -= smax*ones(typeof(s))\n    s = exp.(s)\n    s *= 1/(sum(s) + exp(0-smax))\n    return(s)\n  end\n  batchlen= length(∫.x)÷B\n  @assert B*batchlen==length(∫.x)\n  out = MVector{B,typeof(δ)}(undef)\n  Polyester.@batch for b ∈ 1:B  \n    batch = ((b-1)*(batchlen)+1):(b*batchlen)\n    out[b] = zero(typeof(δ))\n    for i ∈ batch\n      out[b] += shareν(∫.x[i])*∫.w[i]\n    end\n  end\n  return(sum(out))\nend\nPolyester.reset_threads!()\n@benchmark share_v4(sδ,sΣ,sX,s∫, Val(20))\nBenchmarkTools.Trial: 10000 samples with 8 evaluations.\n Range (min … max):  4.327 μs …  1.581 ms  ┊ GC (min … max): 0.00% … 37.99%\n Time  (median):     4.769 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   5.510 μs ± 22.203 μs  ┊ GC (mean ± σ):  2.16% ±  0.53%\n\n       ▄▆█▇▄▁                                 ▁▂▁             \n  ▁▂▂▅███████▇▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▄▆▇████▇▆▄▄▃▂▂▁▁▁ ▃\n  4.33 μs        Histogram: frequency by time        6.39 μs &lt;\n\n Memory estimate: 1.55 KiB, allocs estimate: 2."
  },
  {
    "objectID": "assignment.html#initial-benchmark-1",
    "href": "assignment.html#initial-benchmark-1",
    "title": "Coding for Performance",
    "section": "Initial Benchmark",
    "text": "Initial Benchmark\n@benchmark klm(gi)(β0)\nBenchmarkTools.Trial: 9747 samples with 1 evaluation.\n Range (min … max):  458.320 μs …  14.717 ms  ┊ GC (min … max): 0.00% … 95.\n33%\n Time  (median):     473.890 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   506.388 μs ± 513.723 μs  ┊ GC (mean ± σ):  5.24% ±  4.\n99%\n\n        ▁▄▇█▇▆▄▂                                                 \n  ▂▂▂▃▄▆█████████▇▆▆▅▅▅▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂ ▃\n  458 μs           Histogram: frequency by time          542 μs &lt;\n\n Memory estimate: 211.50 KiB, allocs estimate: 2759."
  },
  {
    "objectID": "assignment.html#fixing-type-instabilities",
    "href": "assignment.html#fixing-type-instabilities",
    "title": "Coding for Performance",
    "section": "Fixing Type Instabilities",
    "text": "Fixing Type Instabilities\nFrom @code_warntype, we see that the compiles is unable to infer the type of some variables. The problem seems to start with D. This is quite puzzling because D is explicitly initialized as zeros(eltype(Gi),...).\njulia&gt; @code_warntype klm(gi)(β0)\nMethodInstance for (::var\"#24#25\"{var\"#21#23\"{var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}, var\"#P#22\"}})(::Vector{Float64})\n  from (::var\"#24#25\")(θ) @ Main ~/.julia/dev/ARGridBootstrap/docs/jmd/assignment.jmd:31\nArguments\n  #self#::var\"#24#25\"{var\"#21#23\"{var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}, var\"#P#22\"}}\n  θ::Vector{Float64}\nLocals\n  @_3::Int64\n  P::var\"#P#22\"\n  D::ANY\n  Ω::Matrix{Float64}\n  gn::Adjoint{Float64, Matrix{Float64}}\n  p::Int64\n  k::Int64\n  n::Int64\nBody::ANY\n1 ─ %1  = Core.getfield(#self#, :SP)::var\"#21#23\"{var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}, var\"#P#22\"}\n│   %2  = (%1)(θ)::TUPLE{INT64, INT64, INT64, ADJOINT{FLOAT64, MATRIX{FLOAT64}}, MATRIX{FLOAT64}, ANY, VAR\"#P#22\"}\n│   %3  = Base.indexed_iterate(%2, 1)::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(2)])\n│         (n = Core.getfield(%3, 1))\n│         (@_3 = Core.getfield(%3, 2))\n│   %6  = Base.indexed_iterate(%2, 2, @_3::Core.Const(2))::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(3)])\n│         (k = Core.getfield(%6, 1))\n│         (@_3 = Core.getfield(%6, 2))\n│   %9  = Base.indexed_iterate(%2, 3, @_3::Core.Const(3))::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(4)])\n│         (p = Core.getfield(%9, 1))\n│         (@_3 = Core.getfield(%9, 2))\n│   %12 = Base.indexed_iterate(%2, 4, @_3::Core.Const(4))::Core.PartialStruct(Tuple{Adjoint{Float64, Matrix{Float64}}, Int64}, Any[Adjoint{Float64, Matrix{Float64}}, Core.Const(5)])\n│         (gn = Core.getfield(%12, 1))\n│         (@_3 = Core.getfield(%12, 2))\n│   %15 = Base.indexed_iterate(%2, 5, @_3::Core.Const(5))::Core.PartialStruct(Tuple{Matrix{Float64}, Int64}, Any[Matrix{Float64}, Core.Const(6)])\n│         (Ω = Core.getfield(%15, 1))\n│         (@_3 = Core.getfield(%15, 2))\n│   %18 = Base.indexed_iterate(%2, 6, @_3::Core.Const(6))::Core.PartialStruct(Tuple{Any, Int64}, Any[Any, Core.Const(7)])\n│         (D = Core.getfield(%18, 1))\n│         (@_3 = Core.getfield(%18, 2))\n│   %21 = Base.indexed_iterate(%2, 7, @_3::Core.Const(7))::Core.PartialStruct(Tuple{var\"#P#22\", Int64}, Any[var\"#P#22\", Core.Const(8)])\n│         (P = Core.getfield(%21, 1))\n│   %23 = n::Int64\n│   %24 = Main.:var\"'\"(gn)::Matrix{Float64}\n│   %25 = Ω::Matrix{Float64}\n│   %26 = (-1 / 2)::Core.Const(-0.5)\n│   %27 = (%25 ^ %26)::ANY\n│   %28 = Ω::Matrix{Float64}\n│   %29 = (-1 / 2)::Core.Const(-0.5)\n│   %30 = (%28 ^ %29)::ANY\n│   %31 = (%30 * D)::ANY\n│   %32 = (P)(%31)::ANY\n│   %33 = Ω::Matrix{Float64}\n│   %34 = (-1 / 2)::Core.Const(-0.5)\n│   %35 = (%33 ^ %34)::ANY\n│   %36 = (%24 * %27 * %32 * %35 * gn)::ANY\n│   %37 = Base.getindex(%36, 1)::ANY\n│   %38 = (%23 * %37)::ANY\n└──       return %38\nTo investigate further, let us focus on statparts.\njulia&gt; @code_warntype statparts(gi)(β0)\nMethodInstance for (::var\"#21#23\"{var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}, var\"#P#22\"})(::Vector{Float64})\n  from (::var\"#21#23\")(θ) @ Main ~/.julia/dev/ARGridBootstrap/docs/jmd/assignment.jmd:7\nArguments\n  #self#::var\"#21#23\"{var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}, var\"#P#22\"}\n  θ::Vector{Float64}\nLocals\n  @_3::UNION{NOTHING, TUPLE{INT64, INT64}}\n  @_4::Int64\n  D::ANY\n  Γ::ANY\n  G::ANY\n  Gi::ANY\n  gn::Adjoint{Float64, Matrix{Float64}}\n  Ω::Matrix{Float64}\n  k::Int64\n  n::Int64\n  p::Int64\n  giθ::Matrix{Float64}\n  @_15::UNION{NOTHING, TUPLE{INT64, INT64}}\n  j::Int64\n  i::Int64\nBody::TUPLE{INT64, INT64, INT64, ADJOINT{FLOAT64, MATRIX{FLOAT64}}, MATRIX{FLOAT64}, ANY, VAR\"#P#22\"}\n1 ─ %1  = Core.getfield(#self#, :gi)::var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}\n│         (giθ = (%1)(θ))\n│         (p = Main.length(θ))\n│   %4  = Main.size(giθ)::Tuple{Int64, Int64}\n│   %5  = Base.indexed_iterate(%4, 1)::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(2)])\n│         (n = Core.getfield(%5, 1))\n│         (@_4 = Core.getfield(%5, 2))\n│   %8  = Base.indexed_iterate(%4, 2, @_4::Core.Const(2))::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(3)])\n│         (k = Core.getfield(%8, 1))\n│         (Ω = Main.cov(giθ))\n│   %11 = Core.getfield(#self#, :gi)::var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}\n│   %12 = (%11)(θ)::Matrix{Float64}\n│   %13 = (:dims,)::Core.Const((:dims,))\n│   %14 = Core.apply_type(Core.NamedTuple, %13)::Core.Const(NamedTuple{(:dims,)})\n│   %15 = Core.tuple(1)::Core.Const((1,))\n│   %16 = (%14)(%15)::Core.Const((dims = 1,))\n│   %17 = Core.kwcall(%16, Main.mean, %12)::Matrix{Float64}\n│         (gn = Main.:var\"'\"(%17))\n│   %19 = ForwardDiff.jacobian::Core.Const(ForwardDiff.jacobian)\n│   %20 = Core.getfield(#self#, :gi)::var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}\n│         (Gi = (%19)(%20, θ))\n│         (Gi = Main.reshape(Gi, n, k, p))\n│   %23 = (:dims,)::Core.Const((:dims,))\n│   %24 = Core.apply_type(Core.NamedTuple, %23)::Core.Const(NamedTuple{(:dims,)})\n│   %25 = Core.tuple(1)::Core.Const((1,))\n│   %26 = (%24)(%25)::Core.Const((dims = 1,))\n│         (G = Core.kwcall(%26, Main.mean, Gi))\n│   %28 = Main.eltype(Gi)::ANY\n│   %29 = p::Int64\n│   %30 = k::Int64\n│         (Γ = Main.zeros(%28, %29, %30, k))\n│   %32 = Main.eltype(Gi)::ANY\n│   %33 = k::Int64\n│         (D = Main.zeros(%32, %33, p))\n│   %35 = (1:p)::Core.PartialStruct(UnitRange{Int64}, Any[Core.Const(1), Int64])\n│         (@_3 = Base.iterate(%35))\n│   %37 = (@_3 === nothing)::Bool\n│   %38 = Base.not_int(%37)::Bool\n└──       goto #7 if not %38\n2 ┄ %40 = @_3::Tuple{Int64, Int64}\n│         (j = Core.getfield(%40, 1))\n│   %42 = Core.getfield(%40, 2)::Int64\n│   %43 = (1:n)::Core.PartialStruct(UnitRange{Int64}, Any[Core.Const(1), Int64])\n│         (@_15 = Base.iterate(%43))\n│   %45 = (@_15 === nothing)::Bool\n│   %46 = Base.not_int(%45)::Bool\n└──       goto #5 if not %46\n3 ┄ %48 = @_15::Tuple{Int64, Int64}\n│         (i = Core.getfield(%48, 1))\n│   %50 = Core.getfield(%48, 2)::Int64\n│   %51 = Base.getindex(Γ, j, Main.:(:), Main.:(:))::ANY\n│   %52 = Main.:-::Core.Const(-)\n│   %53 = Base.getindex(Gi, i, Main.:(:), j)::ANY\n│   %54 = Base.getindex(G, 1, Main.:(:), j)::ANY\n│   %55 = Base.broadcasted(%52, %53, %54)::ANY\n│   %56 = Base.materialize(%55)::ANY\n│   %57 = Base.getindex(giθ, i, Main.:(:))::Vector{Float64}\n│   %58 = Main.:var\"'\"(%57)::Adjoint{Float64, Vector{Float64}}\n│   %59 = (%56 * %58)::ANY\n│   %60 = (%51 + %59)::ANY\n│         Base.setindex!(Γ, %60, j, Main.:(:), Main.:(:))\n│         (@_15 = Base.iterate(%43, %50))\n│   %63 = (@_15 === nothing)::Bool\n│   %64 = Base.not_int(%63)::Bool\n└──       goto #5 if not %64\n4 ─       goto #3\n5 ┄ %67 = Base.dotview(Γ, j, Main.:(:), Main.:(:))::ANY\n│   %68 = Main.:/::Core.Const(/)\n│   %69 = Base.getindex(Γ, j, Main.:(:), Main.:(:))::ANY\n│   %70 = Base.broadcasted(%68, %69, n)::ANY\n│         Base.materialize!(%67, %70)\n│   %72 = Base.getindex(G, 1, Main.:(:), j)::ANY\n│   %73 = Base.getindex(Γ, j, Main.:(:), Main.:(:))::ANY\n│   %74 = Main.inv(Ω)::Matrix{Float64}\n│   %75 = (%73 * %74 * gn)::ANY\n│   %76 = (%72 - %75)::ANY\n│         Base.setindex!(D, %76, Main.:(:), j)\n│         (@_3 = Base.iterate(%35, %42))\n│   %79 = (@_3 === nothing)::Bool\n│   %80 = Base.not_int(%79)::Bool\n└──       goto #7 if not %80\n6 ─       goto #2\n7 ┄ %83 = n::Int64\n│   %84 = k::Int64\n│   %85 = p::Int64\n│   %86 = gn::Adjoint{Float64, Matrix{Float64}}\n│   %87 = Ω::Matrix{Float64}\n│   %88 = D::ANY\n│   %89 = Core.getfield(#self#, :P)::Core.Const(var\"#P#22\"())\n│   %90 = Core.tuple(%83, %84, %85, %86, %87, %88, %89)::TUPLE{INT64, INT64, INT64, ADJOINT{FLOAT64, MATRIX{FLOAT64}}, MATRIX{FLOAT64}, ANY, VAR\"#P#22\"}\n└──       return %90\nWe see that G, Gi, Γ, and D are all type Any. For some reason, the return value of ForwardDiff.jacobian is not being inferred. We can workaround this by using an jacobian! instead.\nfunction statparts(gi::F) where {F &lt;: Function}\n  function P(A::AbstractMatrix) # projection matrix\n    A*pinv(A'*A)*A'\n  end\n  let gi=gi\n    function(θ)\n      giθ = gi(θ)\n      p = length(θ)\n      (n, k) = size(giθ)\n      Ω = Hermitian(cov(giθ))\n      gn=mean(gi(θ), dims=1)'\n      Gi = zeros(n,k,p)\n      ForwardDiff.jacobian!(Gi,gi,θ)\n      Gi = reshape(Gi, n , k, p)\n      G = mean(Gi, dims=1)\n      Γ = zeros(eltype(Gi),p,k,k)\n      D = zeros(eltype(Gi),k, p)\n      for j in 1:p\n        for i in 1:n\n          Γ[j,:,:] += (Gi[i,:,j] .- G[1,:,j]) * giθ[i,:]'\n        end\n        Γ[j,:,:] ./= n\n        D[:,j] = G[1,:,j] - Γ[j,:,:]*inv(Ω)*gn\n      end\n      return(n,k,p,gn, Ω, D, P)\n    end\n  end\nend\nstatparts (generic function with 1 method)\njulia&gt; @code_warntype statparts(gi)(β0)\nMethodInstance for (::var\"#28#30\"{var\"#P#29\", var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}})(::Vector{Float64})\n  from (::var\"#28#30\")(θ) @ Main ~/.julia/dev/ARGridBootstrap/docs/jmd/assignment.jmd:7\nArguments\n  #self#::var\"#28#30\"{var\"#P#29\", var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}}\n  θ::Vector{Float64}\nLocals\n  @_3::UNION{NOTHING, TUPLE{INT64, INT64}}\n  @_4::Int64\n  D::Matrix{Float64}\n  Γ::Array{Float64, 3}\n  G::Array{Float64, 3}\n  Gi::Array{Float64, 3}\n  gn::Adjoint{Float64, Matrix{Float64}}\n  Ω::Hermitian{Float64, Matrix{Float64}}\n  k::Int64\n  n::Int64\n  p::Int64\n  giθ::Matrix{Float64}\n  @_15::UNION{NOTHING, TUPLE{INT64, INT64}}\n  j::Int64\n  i::Int64\nBody::Tuple{Int64, Int64, Int64, Adjoint{Float64, Matrix{Float64}}, Hermitian{Float64, Matrix{Float64}}, Matrix{Float64}, var\"#P#29\"}\n1 ─ %1  = Core.getfield(#self#, Symbol(\"#457#gi\"))::var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}\n│         (giθ = (%1)(θ))\n│         (p = Main.length(θ))\n│   %4  = Main.size(giθ)::Tuple{Int64, Int64}\n│   %5  = Base.indexed_iterate(%4, 1)::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(2)])\n│         (n = Core.getfield(%5, 1))\n│         (@_4 = Core.getfield(%5, 2))\n│   %8  = Base.indexed_iterate(%4, 2, @_4::Core.Const(2))::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(3)])\n│         (k = Core.getfield(%8, 1))\n│   %10 = Main.cov(giθ)::Matrix{Float64}\n│         (Ω = Main.Hermitian(%10))\n│   %12 = Core.getfield(#self#, Symbol(\"#457#gi\"))::var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}\n│   %13 = (%12)(θ)::Matrix{Float64}\n│   %14 = (:dims,)::Core.Const((:dims,))\n│   %15 = Core.apply_type(Core.NamedTuple, %14)::Core.Const(NamedTuple{(:dims,)})\n│   %16 = Core.tuple(1)::Core.Const((1,))\n│   %17 = (%15)(%16)::Core.Const((dims = 1,))\n│   %18 = Core.kwcall(%17, Main.mean, %13)::Matrix{Float64}\n│         (gn = Main.:var\"'\"(%18))\n│         (Gi = Main.zeros(n, k, p))\n│   %21 = ForwardDiff.jacobian!::Core.Const(ForwardDiff.jacobian!)\n│   %22 = Gi::Array{Float64, 3}\n│   %23 = Core.getfield(#self#, Symbol(\"#457#gi\"))::var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}\n│         (%21)(%22, %23, θ)\n│         (Gi = Main.reshape(Gi, n, k, p))\n│   %26 = (:dims,)::Core.Const((:dims,))\n│   %27 = Core.apply_type(Core.NamedTuple, %26)::Core.Const(NamedTuple{(:dims,)})\n│   %28 = Core.tuple(1)::Core.Const((1,))\n│   %29 = (%27)(%28)::Core.Const((dims = 1,))\n│         (G = Core.kwcall(%29, Main.mean, Gi))\n│   %31 = Main.eltype(Gi)::Core.Const(Float64)\n│   %32 = p::Int64\n│   %33 = k::Int64\n│         (Γ = Main.zeros(%31, %32, %33, k))\n│   %35 = Main.eltype(Gi)::Core.Const(Float64)\n│   %36 = k::Int64\n│         (D = Main.zeros(%35, %36, p))\n│   %38 = (1:p)::Core.PartialStruct(UnitRange{Int64}, Any[Core.Const(1), Int64])\n│         (@_3 = Base.iterate(%38))\n│   %40 = (@_3 === nothing)::Bool\n│   %41 = Base.not_int(%40)::Bool\n└──       goto #7 if not %41\n2 ┄ %43 = @_3::Tuple{Int64, Int64}\n│         (j = Core.getfield(%43, 1))\n│   %45 = Core.getfield(%43, 2)::Int64\n│   %46 = (1:n)::Core.PartialStruct(UnitRange{Int64}, Any[Core.Const(1), Int64])\n│         (@_15 = Base.iterate(%46))\n│   %48 = (@_15 === nothing)::Bool\n│   %49 = Base.not_int(%48)::Bool\n└──       goto #5 if not %49\n3 ┄ %51 = @_15::Tuple{Int64, Int64}\n│         (i = Core.getfield(%51, 1))\n│   %53 = Core.getfield(%51, 2)::Int64\n│   %54 = Base.getindex(Γ, j, Main.:(:), Main.:(:))::Matrix{Float64}\n│   %55 = Main.:-::Core.Const(-)\n│   %56 = Base.getindex(Gi, i, Main.:(:), j)::Vector{Float64}\n│   %57 = Base.getindex(G, 1, Main.:(:), j)::Vector{Float64}\n│   %58 = Base.broadcasted(%55, %56, %57)::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Nothing, typeof(-), Tuple{Vector{Float64}, Vector{Float64}}}\n│   %59 = Base.materialize(%58)::Vector{Float64}\n│   %60 = Base.getindex(giθ, i, Main.:(:))::Vector{Float64}\n│   %61 = Main.:var\"'\"(%60)::Adjoint{Float64, Vector{Float64}}\n│   %62 = (%59 * %61)::Matrix{Float64}\n│   %63 = (%54 + %62)::Matrix{Float64}\n│         Base.setindex!(Γ, %63, j, Main.:(:), Main.:(:))\n│         (@_15 = Base.iterate(%46, %53))\n│   %66 = (@_15 === nothing)::Bool\n│   %67 = Base.not_int(%66)::Bool\n└──       goto #5 if not %67\n4 ─       goto #3\n5 ┄ %70 = Base.dotview(Γ, j, Main.:(:), Main.:(:))::SubArray{Float64, 2, Array{Float64, 3}, Tuple{Int64, Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true}\n│   %71 = Main.:/::Core.Const(/)\n│   %72 = Base.getindex(Γ, j, Main.:(:), Main.:(:))::Matrix{Float64}\n│   %73 = Base.broadcasted(%71, %72, n)::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2}, Nothing, typeof(/), Tuple{Matrix{Float64}, Int64}}\n│         Base.materialize!(%70, %73)\n│   %75 = Base.getindex(G, 1, Main.:(:), j)::Vector{Float64}\n│   %76 = Base.getindex(Γ, j, Main.:(:), Main.:(:))::Matrix{Float64}\n│   %77 = Main.inv(Ω::Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')]))::Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')])\n│   %78 = (%76 * %77 * gn)::Matrix{Float64}\n│   %79 = (%75 - %78)::Matrix{Float64}\n│         Base.setindex!(D, %79, Main.:(:), j)\n│         (@_3 = Base.iterate(%38, %45))\n│   %82 = (@_3 === nothing)::Bool\n│   %83 = Base.not_int(%82)::Bool\n└──       goto #7 if not %83\n6 ─       goto #2\n7 ┄ %86 = n::Int64\n│   %87 = k::Int64\n│   %88 = p::Int64\n│   %89 = gn::Adjoint{Float64, Matrix{Float64}}\n│   %90 = Ω::Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')])\n│   %91 = D::Matrix{Float64}\n│   %92 = Core.getfield(#self#, :P)::Core.Const(var\"#P#29\"())\n│   %93 = Core.tuple(%86, %87, %88, %89, %90, %91, %92)::Core.PartialStruct(Tuple{Int64, Int64, Int64, Adjoint{Float64, Matrix{Float64}}, Hermitian{Float64, Matrix{Float64}}, Matrix{Float64}, var\"#P#29\"}, Any[Int64, Int64, Int64, Adjoint{Float64, Matrix{Float64}}, Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')]), Matrix{Float64}, var\"#P#29\"])\n└──       return %93\nI also added the where {F statement to ensure compiler specialization, and added the let gi=gi line to help with the performance of captured variables.\njulia&gt; @code_warntype klm(gi)(β0)\nMethodInstance for (::var\"#24#25\"{var\"#28#30\"{var\"#P#29\", var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}}})(::Vector{Float64})\n  from (::var\"#24#25\")(θ) @ Main ~/.julia/dev/ARGridBootstrap/docs/jmd/assignment.jmd:31\nArguments\n  #self#::var\"#24#25\"{var\"#28#30\"{var\"#P#29\", var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}}}\n  θ::Vector{Float64}\nLocals\n  @_3::Int64\n  P::var\"#P#29\"\n  D::Matrix{Float64}\n  Ω::Hermitian{Float64, Matrix{Float64}}\n  gn::Adjoint{Float64, Matrix{Float64}}\n  p::Int64\n  k::Int64\n  n::Int64\nBody::UNION{FLOAT64, COMPLEXF64}\n1 ─ %1  = Core.getfield(#self#, :SP)::var\"#28#30\"{var\"#P#29\", var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}}\n│   %2  = (%1)(θ)::Core.PartialStruct(Tuple{Int64, Int64, Int64, Adjoint{Float64, Matrix{Float64}}, Hermitian{Float64, Matrix{Float64}}, Matrix{Float64}, var\"#P#29\"}, Any[Int64, Int64, Int64, Adjoint{Float64, Matrix{Float64}}, Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')]), Matrix{Float64}, var\"#P#29\"])\n│   %3  = Base.indexed_iterate(%2, 1)::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(2)])\n│         (n = Core.getfield(%3, 1))\n│         (@_3 = Core.getfield(%3, 2))\n│   %6  = Base.indexed_iterate(%2, 2, @_3::Core.Const(2))::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(3)])\n│         (k = Core.getfield(%6, 1))\n│         (@_3 = Core.getfield(%6, 2))\n│   %9  = Base.indexed_iterate(%2, 3, @_3::Core.Const(3))::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(4)])\n│         (p = Core.getfield(%9, 1))\n│         (@_3 = Core.getfield(%9, 2))\n│   %12 = Base.indexed_iterate(%2, 4, @_3::Core.Const(4))::Core.PartialStruct(Tuple{Adjoint{Float64, Matrix{Float64}}, Int64}, Any[Adjoint{Float64, Matrix{Float64}}, Core.Const(5)])\n│         (gn = Core.getfield(%12, 1))\n│         (@_3 = Core.getfield(%12, 2))\n│   %15 = Base.indexed_iterate(%2, 5, @_3::Core.Const(5))::Core.PartialStruct(Tuple{Hermitian{Float64, Matrix{Float64}}, Int64}, Any[Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')]), Core.Const(6)])\n│         (Ω = Core.getfield(%15, 1))\n│         (@_3 = Core.getfield(%15, 2))\n│   %18 = Base.indexed_iterate(%2, 6, @_3::Core.Const(6))::Core.PartialStruct(Tuple{Matrix{Float64}, Int64}, Any[Matrix{Float64}, Core.Const(7)])\n│         (D = Core.getfield(%18, 1))\n│         (@_3 = Core.getfield(%18, 2))\n│   %21 = Base.indexed_iterate(%2, 7, @_3::Core.Const(7))::Core.PartialStruct(Tuple{var\"#P#29\", Int64}, Any[var\"#P#29\", Core.Const(8)])\n│         (P = Core.getfield(%21, 1))\n│   %23 = n::Int64\n│   %24 = Main.:var\"'\"(gn)::Matrix{Float64}\n│   %25 = Ω::Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')])\n│   %26 = (-1 / 2)::Core.Const(-0.5)\n│   %27 = (%25 ^ %26)::UNION{HERMITIAN{FLOAT64, MATRIX{FLOAT64}}, MATRIX{COMPLEXF64}}\n│   %28 = Ω::Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')])\n│   %29 = (-1 / 2)::Core.Const(-0.5)\n│   %30 = (%28 ^ %29)::UNION{HERMITIAN{FLOAT64, MATRIX{FLOAT64}}, MATRIX{COMPLEXF64}}\n│   %31 = (%30 * D)::UNION{MATRIX{COMPLEXF64}, MATRIX{FLOAT64}}\n│   %32 = (P)(%31)::UNION{MATRIX{COMPLEXF64}, MATRIX{FLOAT64}}\n│   %33 = Ω::Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')])\n│   %34 = (-1 / 2)::Core.Const(-0.5)\n│   %35 = (%33 ^ %34)::UNION{HERMITIAN{FLOAT64, MATRIX{FLOAT64}}, MATRIX{COMPLEXF64}}\n│   %36 = (%24 * %27 * %32 * %35 * gn)::UNION{MATRIX{COMPLEXF64}, MATRIX{FLOAT64}}\n│   %37 = Base.getindex(%36, 1)::UNION{FLOAT64, COMPLEXF64}\n│   %38 = (%23 * %37)::UNION{FLOAT64, COMPLEXF64}\n└──       return %38\nThere’s still a type-instability in klm. This one is harder to understand. It is due to the fact that the appropriate meaning of a matrix square root depends on the nature of the matrix. In particular, the value could be a complex valued matrix instead of real valued. We know that Ω should be positive definite with a real matrix square root. We can compute its square root from its Eigen decomposition and avoid the type instability.\nfunction klm(gi::F ) where {F &lt;: Function}\n  let gi=gi\n    function(θ)\n      (n,k,p,gn, Ω, D, P) = statparts(gi)(θ)\n      λ, v = eigen(Ω)\n      irΩ = v*diagm(λ.^(-1/2))*v'\n      return n*(gn'*irΩ*P(irΩ*D)*irΩ*gn)[1]\n    end\n  end\nend\nklm (generic function with 1 method)\njulia&gt; @code_warntype klm(gi)(β0)\nMethodInstance for (::var\"#31#32\"{var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}})(::Vector{Float64})\n  from (::var\"#31#32\")(θ) @ Main ~/.julia/dev/ARGridBootstrap/docs/jmd/assignment.jmd:4\nArguments\n  #self#::var\"#31#32\"{var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}}\n  θ::Vector{Float64}\nLocals\n  @_3::Val{:vectors}\n  @_4::Int64\n  irΩ::Matrix{Float64}\n  v::Matrix{Float64}\n  λ::Vector{Float64}\n  P::var\"#P#29\"\n  D::Matrix{Float64}\n  Ω::Hermitian{Float64, Matrix{Float64}}\n  gn::Adjoint{Float64, Matrix{Float64}}\n  p::Int64\n  k::Int64\n  n::Int64\nBody::Float64\n1 ─ %1  = Core.getfield(#self#, Symbol(\"#458#gi\"))::var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}\n│   %2  = Main.statparts(%1)::var\"#28#30\"{var\"#P#29\", var\"#26#27\"{Vector{Float64}, Matrix{Float64}, Matrix{Float64}}}\n│   %3  = (%2)(θ)::Core.PartialStruct(Tuple{Int64, Int64, Int64, Adjoint{Float64, Matrix{Float64}}, Hermitian{Float64, Matrix{Float64}}, Matrix{Float64}, var\"#P#29\"}, Any[Int64, Int64, Int64, Adjoint{Float64, Matrix{Float64}}, Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')]), Matrix{Float64}, var\"#P#29\"])\n│   %4  = Base.indexed_iterate(%3, 1)::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(2)])\n│         (n = Core.getfield(%4, 1))\n│         (@_4 = Core.getfield(%4, 2))\n│   %7  = Base.indexed_iterate(%3, 2, @_4::Core.Const(2))::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(3)])\n│         (k = Core.getfield(%7, 1))\n│         (@_4 = Core.getfield(%7, 2))\n│   %10 = Base.indexed_iterate(%3, 3, @_4::Core.Const(3))::Core.PartialStruct(Tuple{Int64, Int64}, Any[Int64, Core.Const(4)])\n│         (p = Core.getfield(%10, 1))\n│         (@_4 = Core.getfield(%10, 2))\n│   %13 = Base.indexed_iterate(%3, 4, @_4::Core.Const(4))::Core.PartialStruct(Tuple{Adjoint{Float64, Matrix{Float64}}, Int64}, Any[Adjoint{Float64, Matrix{Float64}}, Core.Const(5)])\n│         (gn = Core.getfield(%13, 1))\n│         (@_4 = Core.getfield(%13, 2))\n│   %16 = Base.indexed_iterate(%3, 5, @_4::Core.Const(5))::Core.PartialStruct(Tuple{Hermitian{Float64, Matrix{Float64}}, Int64}, Any[Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')]), Core.Const(6)])\n│         (Ω = Core.getfield(%16, 1))\n│         (@_4 = Core.getfield(%16, 2))\n│   %19 = Base.indexed_iterate(%3, 6, @_4::Core.Const(6))::Core.PartialStruct(Tuple{Matrix{Float64}, Int64}, Any[Matrix{Float64}, Core.Const(7)])\n│         (D = Core.getfield(%19, 1))\n│         (@_4 = Core.getfield(%19, 2))\n│   %22 = Base.indexed_iterate(%3, 7, @_4::Core.Const(7))::Core.PartialStruct(Tuple{var\"#P#29\", Int64}, Any[var\"#P#29\", Core.Const(8)])\n│         (P = Core.getfield(%22, 1))\n│   %24 = Main.eigen(Ω::Core.PartialStruct(Hermitian{Float64, Matrix{Float64}}, Any[Matrix{Float64}, Core.Const('U')]))::Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\n│   %25 = Base.indexed_iterate(%24, 1)::Tuple{Vector{Float64}, Val{:vectors}}\n│         (λ = Core.getfield(%25, 1))\n│         (@_3 = Core.getfield(%25, 2))\n│   %28 = Base.indexed_iterate(%24, 2, @_3)::Tuple{Matrix{Float64}, Val{:done}}\n│         (v = Core.getfield(%28, 1))\n│   %30 = v::Matrix{Float64}\n│   %31 = Main.:^::Core.Const(^)\n│   %32 = λ::Vector{Float64}\n│   %33 = (-1 / 2)::Core.Const(-0.5)\n│   %34 = Base.broadcasted(%31, %32, %33)::Core.PartialStruct(Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Nothing, typeof(^), Tuple{Vector{Float64}, Float64}}, Any[Core.Const(Base.Broadcast.DefaultArrayStyle{1}()), Core.Const(^), Core.PartialStruct(Tuple{Vector{Float64}, Float64}, Any[Vector{Float64}, Core.Const(-0.5)]), Nothing])\n│   %35 = Base.materialize(%34)::Vector{Float64}\n│   %36 = Main.diagm(%35)::Matrix{Float64}\n│   %37 = Main.:var\"'\"(v)::Adjoint{Float64, Matrix{Float64}}\n│         (irΩ = %30 * %36 * %37)\n│   %39 = n::Int64\n│   %40 = Main.:var\"'\"(gn)::Matrix{Float64}\n│   %41 = irΩ::Matrix{Float64}\n│   %42 = (irΩ * D)::Matrix{Float64}\n│   %43 = (P)(%42)::Matrix{Float64}\n│   %44 = irΩ::Matrix{Float64}\n│   %45 = (%40 * %41 * %43 * %44 * gn)::Matrix{Float64}\n│   %46 = Base.getindex(%45, 1)::Float64\n│   %47 = (%39 * %46)::Float64\n└──       return %47\nFixing these type instabilities speeds up the code by a factor of about 5.\n@benchmark klm(gi)(β0)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  132.812 μs …   9.251 ms  ┊ GC (min … max):  0.00% … 90\n.89%\n Time  (median):     148.332 μs               ┊ GC (median):     0.00%\n Time  (mean ± σ):   174.327 μs ± 435.225 μs  ┊ GC (mean ± σ):  12.87% ±  5\n.07%\n\n                ▃▆██▅▂                                           \n  ▂▂▂▂▂▂▃▄▄▄▅▅▆████████▇▆▆▅▅▄▄▄▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂ ▃\n  133 μs           Histogram: frequency by time          186 μs &lt;\n\n Memory estimate: 219.61 KiB, allocs estimate: 1516."
  },
  {
    "objectID": "assignment.html#reducing-allocations-and-other-optimizations",
    "href": "assignment.html#reducing-allocations-and-other-optimizations",
    "title": "Coding for Performance",
    "section": "Reducing allocations and Other Optimizations",
    "text": "Reducing allocations and Other Optimizations\nProfiling reveals the majority of time is spent in the innermost loop of the statparts function. This loop allocates quite a bit because the arrays are using slices. We can avoid allocations by using @views and more broadcasting. See “Consider using views for slices” and “More dots”,\nfunction statparts(gi::F) where {F &lt;: Function}\n  function P(A::AbstractMatrix) # projection matrix\n    A*pinv(A'*A)*A'\n  end\n  let gi=gi\n    function(θ)\n      giθ = gi(θ)\n      p = length(θ)\n      (n, k) = size(giθ)\n      Ω = Hermitian(cov(giθ))\n      gn=mean(gi(θ), dims=1)'\n      iΩgn = Ω \\ gn\n      Gi = zeros(n,k,p)\n      ForwardDiff.jacobian!(Gi,gi,θ)\n      Gi = reshape(Gi, n , k, p)\n      G = mean(Gi, dims=1)\n      Γ = zeros(eltype(Gi),p,k,k)\n      D = zeros(eltype(Gi),k, p)\n      @inbounds for j in 1:p\n        @inbounds for i in 1:n\n          @views Γ[j,:,:] .+= (Gi[i,:,j] .- G[1,:,j]) * giθ[i,:]'\n        end\n        Γ[j,:,:] ./= n\n        @views D[:,j] .= G[1,:,j] .- Γ[j,:,:]*iΩgn\n      end\n      return(n,k,p,gn, Ω, D, P)\n    end\n  end\nend\n@benchmark klm(gi)(β0)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  69.363 μs …   6.704 ms  ┊ GC (min … max):  0.00% … 92.\n90%\n Time  (median):     76.354 μs               ┊ GC (median):     0.00%\n Time  (mean ± σ):   86.922 μs ± 255.393 μs  ┊ GC (mean ± σ):  11.10% ±  3.\n73%\n\n               ▁▁▃▄▆▅▇▇▆█▆▇▄▃▂                                  \n  ▁▁▁▁▁▂▂▃▃▄▅▆█████████████████▇▆▅▄▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▃\n  69.4 μs         Histogram: frequency by time           89 μs &lt;\n\n Memory estimate: 118.53 KiB, allocs estimate: 501.\nThe code is now about ten times faster than the original."
  },
  {
    "objectID": "assignment.html#footnotes",
    "href": "assignment.html#footnotes",
    "title": "Coding for Performance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLike scalar variables, StaticArrays exist on the stack instead of the heap, so creating them is much less costly and they do not count toward the reported allocations.↩︎"
  },
  {
    "objectID": "argridboot.html",
    "href": "argridboot.html",
    "title": "Coding for Performance",
    "section": "",
    "text": "Today we will look into some methods to improve the speed of our code. Although speed is sometimes important, never forget that speed should be low on your list of priorities when writing code. You should prioritize correctness and maintainability ahead of performance. Nonetheless, performance does matter for some problems.\nIf you have not already, be sure to read the Peformance Tips section of Julia Docs.\nAlso, read Rackauckas’s notes on “Optimizing Serial Code.” (Rackauckas 2019)."
  },
  {
    "objectID": "argridboot.html#initial-benchmark-and-profile",
    "href": "argridboot.html#initial-benchmark-and-profile",
    "title": "Coding for Performance",
    "section": "Initial Benchmark and Profile",
    "text": "Initial Benchmark and Profile\nbenchorig=@benchmark (b,t) = gridbootstrap(wrapper(b_est_original), a-&gt;ar1_original(y0, a, est.e),\n                             αgrid, nboot)\nBenchmarkTools.Trial: 38 samples with 1 evaluation.\n Range (min … max):  130.823 ms … 149.250 ms  ┊ GC (min … max): 9.70% … 8.5\n6%\n Time  (median):     131.580 ms               ┊ GC (median):    9.72%\n Time  (mean ± σ):   132.501 ms ±   3.666 ms  ┊ GC (mean ± σ):  9.77% ± 0.5\n2%\n\n  ▃█▃▅▂                                                          \n  █████▄▁▄▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▄ ▁\n  131 ms           Histogram: frequency by time          149 ms &lt;\n\n Memory estimate: 219.44 MiB, allocs estimate: 345075.\nTo make code faster, we should begin by profiling.\n# only to make profile results show nicely in generated html\n# for interactive use, just use @profview in place of @profile\nfunction profilehtmlstring() \n  buf = IOBuffer()\n  show(buf, MIME(\"text/html\"), ProfileCanvas.view(Profile.fetch()))\n  s=String(take!(buf))\n  println(\"\\n\\n\"*s*\"\\n\")\nend\nprofilehtmlstring (generic function with 1 method)\nusing Profile, ProfileCanvas\nProfile.clear();\nProfile.init(n=10^7,delay=0.0001);\n@profile (b,t) = gridbootstrap(wrapper(b_est_original), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 999)\nprofilehtmlstring()\n\n\n\n\nThe profiler works very simply. Every 0.0001 seconds, the line of code being executed gets recorded. There are then various tools for printing and visualizing the results. The @profview macro shows a flame graph. The default view might be a bit strange. The base of the flame graph often includes Julia’s repl and various other things that can be ignored. If you click on the graph, it will zoom in. You can use mouse wheel up to zoom back out.\n\n\n\n\n\n\nTip\n\n\n\nIn VSCode, you can just use @profview in place of @profile and the profile flamegraph will open in a panel within VSCode."
  },
  {
    "objectID": "argridboot.html#removing-redudant-operations",
    "href": "argridboot.html#removing-redudant-operations",
    "title": "Coding for Performance",
    "section": "Removing Redudant Operations",
    "text": "Removing Redudant Operations\n\n\n\n\n\n\nWarning\n\n\n\nThe following was true on an older version of Julia, but now there are no gains from eliminating inv.\n\n\nFrom the output (these numbers can vary quite a bit from run to run), we see there were 640 ticks in gridbootstrap_original (exact numbers will vary on each execution, but relative ones should be similar), and almost all of these occurred within inv. If we want the code to be faster, we should focus on these lines. Calling both inv and \\ is redundant; we should combine these computations.\ns = @code_string b_est_mldivide(y)\ncode_md(s)\nfunction b_est_mldivide(yin)\n  T = length(yin)\n  x = [ones(T-1) 2:T yin[1:(T-1)]]\n  y = yin[2:T]\n  tmp = x'*x \\ [x'*y I]\n  θ = tmp[:,1]\n  ixx = tmp[:,2:4]\n  e = y - x*θ\n  se = sqrt.(diag(ixx *(e'*e))./(T-4))\n  (θ=θ,se=se,e=e)\nend\nbenchml=@benchmark (b,t) = gridbootstrap(wrapper(b_est_mldivide), a-&gt;ar1_original(y0, a, est.e),\n                             αgrid, nboot)\nBenchmarkTools.Trial: 27 samples with 1 evaluation.\n Range (min … max):  179.880 ms … 199.238 ms  ┊ GC (min … max): 6.97% … 6.4\n5%\n Time  (median):     186.427 ms               ┊ GC (median):    8.51%\n Time  (mean ± σ):   185.857 ms ±   3.287 ms  ┊ GC (mean ± σ):  8.20% ± 0.7\n1%\n\n            ▂ ▂        █▅                                        \n  ▅▁▁▁▁▁▁▁▁▁████▁▁▁▁▁▅████▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅ ▁\n  180 ms           Histogram: frequency by time          199 ms &lt;\n\n Memory estimate: 210.46 MiB, allocs estimate: 517608.\nFrom this, we get a speedup by about a factor of 4 on my computer. This used to make a big difference, but no longer seems to matter."
  },
  {
    "objectID": "argridboot.html#reducing-allocations",
    "href": "argridboot.html#reducing-allocations",
    "title": "Coding for Performance",
    "section": "Reducing Allocations",
    "text": "Reducing Allocations\nProfile.clear()\n@profile (b,t) = gridbootstrap(wrapper(b_est_mldivide), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 999)\nprofilehtmlstring()\n\n\n\n\nNow, the most time consuming parts of the code are, unsurprisingly, the call to \\, and, perhaps surprisingly, hcat from creating x. Allocating and copying memory is relatively slow. The creation of x involves both.\n\nCaching Intermediate Arrays\nOne option is to preallocate an arrays and reuse them. The struct bEstCache does this.\nb_est_cache = ARGridBootstrap.bEstCached(T-1)\nbenchcache=@benchmark gridbootstrap(wrapper(b_est_cache), a-&gt;ar1_original(y0, a, est.e), \n                                    αgrid, nboot)\nBenchmarkTools.Trial: 36 samples with 1 evaluation.\n Range (min … max):  138.756 ms … 156.013 ms  ┊ GC (min … max): 7.03% … 6.2\n0%\n Time  (median):     139.455 ms               ┊ GC (median):    7.03%\n Time  (mean ± σ):   140.950 ms ±   3.298 ms  ┊ GC (mean ± σ):  7.63% ± 1.0\n3%\n\n  ▃▅█                                                            \n  ███▁▅▁▁▄▁▁▁▁▄▅▅▇▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄ ▁\n  139 ms           Histogram: frequency by time          156 ms &lt;\n\n Memory estimate: 143.71 MiB, allocs estimate: 162394.\nProfile.clear()\n@profview (b,t) = gridbootstrap(wrapper(b_est_cache), a-&gt;ar1_original(y0, a, est.e),\n                               αgrid, 999)\nprofilehtmlstring()\n\n\n\n\n\n\nEliminating Intermediate Arrays\nBetter yet, we can avoid creating x by just accumulating \\(X'y\\) and \\(X'X\\) in a loop.\ns = @code_string b_est_nox(y)\ncode_md(s)\nfunction b_est_nox(yin; xx_xy!::F=xx_xy!, resids!::FR=resids!) where {F&lt;:Function, FR&lt;:Function}\n  T = length(yin)\n  xx = @MMatrix zeros(eltype(yin),3,3)\n  xy = @MVector zeros(eltype(yin),3)\n  xx_xy!(xx,xy,yin)\n  ixx = inv(xx)\n  θ = ixx * xy\n  e = similar(yin,T-1)\n  resids!(e,yin,θ)\n  se = sqrt.(diag(ixx *(e'*e))./(T-4))\n  (θ=θ,se=se,e=e)\nend\nWe put the two main loops into separate functions both for organization and to allow us to focus on optimizing these loops below.\ns=@code_string ARGridBootstrap.xx_xy!(zeros(3,3),zeros(3),y)\ncode_md(s)\n@inline function xx_xy!(xx,xy,yin)\n  T = length(yin)\n  xx .= zero(eltype(xx))\n  xy .= zero(eltype(xy))\n  @inbounds @simd for t in 2:T\n    xx[1,3] += yin[t-1]\n    xx[2,3] += t*yin[t-1]\n    xx[3,3] += yin[t-1]^2\n    xy[1] += yin[t]\n    xy[2] += t*yin[t]\n    xy[3] += yin[t-1]*yin[t]\n  end\n  xx[1,1] = T-1 # = 1'*1\n  xx[1,2] = xx[2,1] = (T+1)*T/2 - 1 # sum(p+1:T)\n  xx[2,2] = (2*(T)+1)*(T)*(T+1)/6 - 1 # sum((p+1:T).^2)\n  xx[3,1] = xx[1,3]\n  xx[3,2] = xx[2,3]\n  nothing\nend\ns=@code_string ARGridBootstrap.resids!(zeros(length(y)-1),y,zeros(3))\ncode_md(s)\n@inline function resids!(e, yin, θ)\n  T = length(yin)\n  @inbounds @simd for t in 2:T\n    e[t-1] = yin[t] - θ[1] - θ[2]*t - θ[3]*yin[t-1]\n  end\n  nothing\nend\nbenchnox=@benchmark (b,t) = gridbootstrap(wrapper(b_est_nox), a-&gt;ar1_original(y0, a, est.e),\n                             αgrid, nboot)\nBenchmarkTools.Trial: 124 samples with 1 evaluation.\n Range (min … max):  37.839 ms … 61.466 ms  ┊ GC (min … max):  7.96% … 4.77\n%\n Time  (median):     41.088 ms              ┊ GC (median):    14.60%\n Time  (mean ± σ):   40.376 ms ±  2.496 ms  ┊ GC (mean ± σ):  12.25% ± 3.29\n%\n\n   ▁                    █▁▂▃                                   \n  ▇█▆▆▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅████▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃ ▃\n  37.8 ms         Histogram: frequency by time        46.4 ms &lt;\n\n Memory estimate: 71.70 MiB, allocs estimate: 101499.\nWe have further cut the time by a factor of two. However, this performance optimization has been costly in terms of readability and extensibility of our code. If we wanted to fit an AR(p) model instead of AR(1), the b_est_nox function would be more difficult to modify than the b_est_mldivide version.\nWe additionally gained some performance by using mutable StaticArrays to hold \\(X'X\\) and \\(X'y\\).\nxx = zeros(3,3)\nxy = zeros(3)\n@benchmark ARGridBootstrap.xx_xy!(xx,xy,y)\nBenchmarkTools.Trial: 10000 samples with 179 evaluations.\n Range (min … max):  594.637 ns …  1.129 μs  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     597.760 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   602.472 ns ± 26.903 ns  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n   █▄                                                          ▁\n  ▆██▇▄▄▁▁▃▁▁▁▁▁▁▁▇▄▄▄▄█▇████▇▆▅▆▅▇▅▁▅▄▄▃▃▃▄▁▄▄▄▅▅▅▆▅▅▅▅▅▅▅▅▅▆ █\n  595 ns        Histogram: log(frequency) by time       706 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\nusing StaticArrays\nxx = @MMatrix zeros(3,3)\nxy = @MVector zeros(3)\n@benchmark ARGridBootstrap.xx_xy!(xx,xy,y)\nBenchmarkTools.Trial: 10000 samples with 181 evaluations.\n Range (min … max):  588.834 ns …  1.172 μs  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     592.696 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   597.046 ns ± 28.276 ns  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n   ▇█▃                      ▁                                  ▁\n  ▅███▅▁▄▁▁▁▁▁▃▁▁▁▁▃▃▁▁▄▇▇▇████▇▇▆▅▄▅▅▃▄▅▃▁▄▃▁▁▄▄▅▅▄▄▄▅▄▃▅▄▄▄▅ █\n  589 ns        Histogram: log(frequency) by time       695 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0."
  },
  {
    "objectID": "argridboot.html#loopvectorization.jl",
    "href": "argridboot.html#loopvectorization.jl",
    "title": "Coding for Performance",
    "section": "LoopVectorization.jl",
    "text": "LoopVectorization.jl\nAs mentioned above, the Julia compiler tries to automactically use SIMD instructions when it is safe to do so. SIMD instructions often change the order of operations, and since floating point math is not exactly commutative. The compiler tries to avoid reordering operations, but this often prevents SIMD use. The macro @simd tells the compiler to not worry about reordering operations and insert SIMD instructions more aggresively. Still, there are some SIMD operations that the compiler will not insert automatically.\nThe LoopVectorization.jl package defines a macro, @turbo that more aggresively inserts SIMD instructions. This can make a large difference for some loops and broadcasts.\ne = zeros(length(y)-1)\nθ = @MVector zeros(3)\n@benchmark ARGridBootstrap.resids!($e,$y,$θ)\nBenchmarkTools.Trial: 10000 samples with 390 evaluations.\n Range (min … max):  246.418 ns … 555.159 ns  ┊ GC (min … max): 0.00% … 0.0\n0%\n Time  (median):     248.031 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   250.314 ns ±  13.983 ns  ┊ GC (mean ± σ):  0.00% ± 0.0\n0%\n\n   █▆▁               ▁                                          ▁\n  ▇███▄▅▁▃▁▃▁▁▁▁▃▃▃▁▄█▄▅▆▇███▇▇▇▅▅▄▄▃▁▄▁▁▄▄▅▄▃▅▆▄▅▄▅▅▄▅▅▅▅▅▄▄▄▅ █\n  246 ns        Histogram: log(frequency) by time        300 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\n@benchmark ARGridBootstrap.resids_turbo!($e,$y,$θ)\nBenchmarkTools.Trial: 10000 samples with 979 evaluations.\n Range (min … max):  60.996 ns … 131.267 ns  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     61.709 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   62.269 ns ±   2.848 ns  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n    ▄█▅▁                 ▃                                  ▁  ▁\n  ▅▅████▇▅▅▁▁▃▃▁▁▁▁▁▁▁▄▅▄██▄▄▃▃▁▁▁▄▃▄▃▁▁▁▁▃▄▄▅▇▇▇▆▆▆▅▅▅▅▆▅▃▅█▆ █\n  61 ns         Histogram: log(frequency) by time      72.5 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\nWe can also write SIMD instructions ourselves. The SIMD.jl package makes this somewhat accessible. For an example, resids_simd uses this package to match performance of the @turbo version.\n@benchmark ARGridBootstrap.resids_simd!($e,$y,$θ, Val(8))\nBenchmarkTools.Trial: 10000 samples with 979 evaluations.\n Range (min … max):  63.778 ns … 120.281 ns  ┊ GC (min … max): 0.00% … 0.00\n%\n Time  (median):     64.349 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   64.973 ns ±   2.993 ns  ┊ GC (mean ± σ):  0.00% ± 0.00\n%\n\n    █▅▂                ▂                                ▁      ▁\n  ▅▆████▁▃▄▃▁▁▁▁▁▁▁▁▁▃▃█▇▄▃▁▁▁▁▄▁▆▆▄▁▄▃▅▅▆▇▇▇▇▆▆▅▅▅▄▅▅▁▄█▆▄▃▅▃ █\n  63.8 ns       Histogram: log(frequency) by time      76.5 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\nGenerally, if @turbo successfully inserted SIMD instructions and made your code substantially faster, it will not be worth your effort to try to manually write SIMD code. However, @turbo will not always be able to insert SIMD instructions. One way to check is through benchmarking. Another way is to inspect @code_llvm ARGridBootstrap.resids_turbo!(e,y,θ). Things like fadd fast &lt;4 x double&gt; are SIMD instructions. The &lt;4 x double&gt; part is the key sign. In contrast, something like %26 = fsub double %24, %25 are scalar instructions.\nThe loops in xx_xy! are not automatically vectorized. Part of the issue is that @turbo cannot tell that xx and xy are statically allocated. If we rewrite the code to use scalars, it would like get vectorized by @turbo. The b_est_stride function does this. However, it is really inconvenient to rewrite array code as scalars. It may be more maintainable to keep the arrays and write SIMD instructions ourselves.\n@benchmark ARGridBootstrap.xx_xy!($xx,$xy,$y)\nBenchmarkTools.Trial: 10000 samples with 183 evaluations.\n Range (min … max):  577.443 ns …  13.795 μs  ┊ GC (min … max): 0.00% … 0.0\n0%\n Time  (median):     580.115 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   586.493 ns ± 134.607 ns  ┊ GC (mean ± σ):  0.00% ± 0.0\n0%\n\n   █▂                   ▁                                       ▁\n  ███▆▄▃▁▃▃▁▃▃▃▃▄▁▁▁▄█▆▇███▇▇▅▅▇▅▁▃▄▄▅▅▄▄▆▄▅▄▄▅▆▄▅▄▅▆▅▄▅▄▆▅▅▄▅▄ █\n  577 ns        Histogram: log(frequency) by time        700 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\n@benchmark ARGridBootstrap.xx_xy_simd!($xx,$xy,$y, Val(16))\nBenchmarkTools.Trial: 10000 samples with 565 evaluations.\n Range (min … max):  206.189 ns … 321.524 ns  ┊ GC (min … max): 0.00% … 0.0\n0%\n Time  (median):     207.427 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   209.219 ns ±   7.418 ns  ┊ GC (mean ± σ):  0.00% ± 0.0\n0%\n\n   ██▂                   ▃▁▁▁        ▁                          ▂\n  ▆███▅▄▃▁▁▁▁▁▁▃▄▃▃▃▄▄▃▄▇█████▇▆▆▅▅▅▇█▆▄▃▄▅▄▅▄▄▅▅██▅▄▃▄▁▄▃▄▄▅▅▆ █\n  206 ns        Histogram: log(frequency) by time        243 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\nThis makes the code faster by a factor of more than 10. The Val(N) argument controls the width of vectors that gets passed to SIMD instructions. The value of N can affect execution by a factor of 5 or more. The best choice of N depends on your exact hardware and the code being executed.\nTo see how much this is worth it, let’s benchmark the full bootstrap code, but using the SIMD versions of resids! and xx_xy!\nb_est_simd = y-&gt;b_est_nox(y, xx_xy! =ARGridBootstrap.xx_xy_simd!, resids! =ARGridBootstrap.resids_simd!)\nbenchsimd=@benchmark (b,t) = gridbootstrap(wrapper(b_est_simd), a-&gt;ar1_original(y0, a, est.e),\n                             αgrid, nboot)\nBenchmarkTools.Trial: 128 samples with 1 evaluation.\n Range (min … max):  36.683 ms … 58.429 ms  ┊ GC (min … max):  8.36% … 5.09\n%\n Time  (median):     39.941 ms              ┊ GC (median):    15.25%\n Time  (mean ± σ):   39.180 ms ±  2.351 ms  ┊ GC (mean ± σ):  12.87% ± 3.57\n%\n\n   ▃                   ▄█▂                                     \n  ██▄▇▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃████▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃ ▃\n  36.7 ms         Histogram: frequency by time        45.4 ms &lt;\n\n Memory estimate: 71.70 MiB, allocs estimate: 101499.\nWe have not improved total execution very much. The problem is that very little of the total time was spent in xx_xy! and resids! to begin with. We did make those functions much faster, but they were such a small portion of total execution time, that it is not noticeable. We should focus our efforts on ar1_original if we want to improve."
  },
  {
    "objectID": "gpu.html",
    "href": "gpu.html",
    "title": "Coding for Performance",
    "section": "",
    "text": "This is"
  },
  {
    "objectID": "gpu.html#array-interface",
    "href": "gpu.html#array-interface",
    "title": "Coding for Performance",
    "section": "Array interface",
    "text": "Array interface\nThe easiest way to use a GPU in Julia is through a high level array interface. ArrayFire.jl, oneAPI.jl, and CUDA.jl each offer such interfaces. We will focus on CUDA.jl in these notes. CUDA.jl relies on Nvidia’s CUDA platform, so it only works with Nvidia GPUs. Nvidia tends to dominate GPGPU, and the GPUs available on cedar.computecanada.ca and in my desktop are Nvidia.\nUsing CUDA.CuArray is simple, but has some limitations. You create arrays on the GPU using CuArray. Any array level operation on these will then be performed efficiently on the GPU. This includes broadcast functions with . and matrix multiplies.\nusing CUDA, Random, BenchmarkTools\n\n\nN = 1000\nM = 1000\n\nfunction cuarraydemo(N,M)\n  # wrapped in a  function so that the CuArrays are freed\n  # otherwise we will run out GPU memory later\n  A = randn(N,M);\n  b = randn(M,2);\n  println(\"Time on CPU\")\n  function foo(A,b)\n    (A.^2)*b\n  end\n  @time c=foo(A,b);\n  @time c=foo(A,b);\n  A_gpu = CuArray(A); # copy of A in GPU memory\n  b_gpu = CuArray(b);\n  println(\"Computations on the GPU are fast\")\n  # @btime does not work inside a function\n  @time CUDA.@sync c_gpu=foo(A_gpu,b_gpu);\n  @time CUDA.@sync c_gpu=foo(A_gpu,b_gpu);\n  println(\"But copying to and from GPU memory is not\")\n  bar(A,b) =Array(foo(CuArray(A), CuArray(b)))\n  @time c2=bar(A,b);\n  @time c2=bar(A,b);\nend\ncuarraydemo (generic function with 1 method)\njulia&gt; cuarraydemo(N,M);\nTime on CPU\n  0.006783 seconds (3 allocations: 7.645 MiB)\n  0.004896 seconds (3 allocations: 7.645 MiB)\nComputations on the GPU are fast\n  0.000337 seconds (80 allocations: 3.125 KiB)\n  0.000214 seconds (80 allocations: 3.125 KiB)\nBut copying to and from GPU memory is not\n  0.001448 seconds (102 allocations: 19.578 KiB)\n  0.001112 seconds (102 allocations: 19.578 KiB)\nCuArrays also allow indexing, so you could use loops and other constructs. However, this will not be fast. CuArrays by itself will be a good method to utilize GPUs when the code is dominated by operations on large arrays.\nUnfortunately, the fastest version of our grid bootstrap code does not fit that description. A loop seems needed to generate \\(y\\) due to the recursiveness of the AR(1) model. The fastest version of the code above involves many operations on small 3x3 arrays.\nEXERCISE: modify b_est_original or b_est_mldivide to utilize CuArrays. The approach taken in those functions involves some moderate sized matrices, so it may benefit from CuArrays."
  },
  {
    "objectID": "gpu.html#custom-cuda-kernels",
    "href": "gpu.html#custom-cuda-kernels",
    "title": "Coding for Performance",
    "section": "Custom CUDA Kernels",
    "text": "Custom CUDA Kernels\nTo parallelize the code above on a GPU, we will have to use a lower level interface to the GPU. To explain how it works, we will begin with a simple example that just squares all the elements of an array.\nDisclaimer: my understanding of CUDA and the inner workings of GPUs is far from complete. Some of the details in this section might be inaccurate.\nA typical workflow with CUDA consists of\n\nAllocate GPU memory and copying arrays into it with CuArray.\nDecide how many threads and what configuration of threads to launch.\nEach thread does some computation by running a “kernel” function.\nCopy result from GPU memory to CPU memory.\n\nIn the code below, 1 happens in cuarray_cudanative_compare, 2 happens in the square! function, square_kernel! is the kernel in 3, and 4 is just not done.\n\nThreads and blocks\nCUDA organizes GPU threads into blocks. I believe that the threads in a block all execute concurrently. Threads in the same block share some memory and registers. All current Nvidia GPUs have a maximum number of threads per block of 1024. Note that threads in the same block share registers1, and different kernel functions will use different numbers of registers at once, so depending on the kernel function, you might be limited to fewer than 1024 threads per block. The number of registers available per block depends on your GPU. You can check your GPU characteristics by compiling and running the C++ program in $CUDA_PATH/samples/1_Utilities/deviceQuery/. Alternatively, you can see this information in Julia by running the code below.\nprintln(\"Maximum threads per block $(attribute(device(), CUDA.CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK))\")\nprintln(\"Maximum x blocks $(attribute(device(), CUDA.CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X))\")\nprintln(\"Maximum registers per block $(attribute(device(), CUDA.CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK))\")\nMaximum threads per block 1024\nMaximum x blocks 2147483647\nMaximum registers per block 65536\nThere is no simple way to predict how many registers a kernel function uses. It will depend both on the code you write and how the compiler optimizes the code. If you encounter cryptic error messages about CUDA resources unavailable, then try reducing the number of threads per block. Alternatively, you can limit the number of registers used by passing the maxregs argument to @cuda.\nYou can execute more than 1024 threads by specifying a number of blocks. There is also a limit to the number of blocks, but it is rather large. In the code below, we set the number of blocks, so that nblocks*nthreads &gt;= length(A). Each thread then operates on a single element of A. When the code is executed, each thread has a unique threadIdx and blockIdx combination, and these are used to assign threads to elements of A. The indices go from 1 to number of threads (or blocks). For convenience you can request threads and blocks to have up 3 dimensions, and there are threadIdx().y and threadIdx().z for the additional dimensions.\nfunction square!(A::CuArray)\n  n = length(A)\n  maxthreads = 1024\n  nthreads = min(maxthreads, n)\n  nblocks  = Int(ceil(n/nthreads))\n\n  @cuda threads=nthreads blocks=nblocks square_kernel!(A)\n\n  return A\nend\n\nfunction square_kernel!(A)\n  i = threadIdx().x + (blockIdx().x-1)*blockDim().x\n  if (i&lt;=length(A))\n    @inbounds A[i] *= A[i]\n  end\n  return nothing # CUDA kernels must return nothing\nend\n\nfunction cuarray_cudanative_compare(A)\n  A_gpu = CuArray(A);\n  println(\"CUDAnative square!\")\n  @time CUDA.@sync square!(A_gpu);\n  @time CUDA.@sync square!(A_gpu);\n\n  println(\"CuArray A*=A\")\n  A_gpu = CuArray(A);\n  @time CUDA.@sync A_gpu .*= A_gpu;\n  @time CUDA.@sync A_gpu .*= A_gpu;\n  return nothing\nend\ncuarray_cudanative_compare (generic function with 1 method)\njulia&gt; cuarray_cudanative_compare(randn(N,M))\nCUDAnative square!\n  0.000142 seconds (8 allocations: 400 bytes)\n  0.000128 seconds (8 allocations: 400 bytes)\nCuArray A*=A\n  0.000220 seconds (31 allocations: 2.109 KiB)\n  0.000135 seconds (31 allocations: 2.109 KiB)\n\n\nKernel Limitations\nCUDA kernel functions execute on the GPU and in GPU memory. Since GPU memory is allocated and managed differently than RAM, many Julia functions will not work in CUDA kernels. Most importantly, Julia functions that allocate dynamically sized arrays will not work. This means that even matrix multiplication like θ = ixx*xy will fail (if ixx or xy are dynamically allocated) since it allocates an array for θ. You can, however, have local scalars, tuples, and StaticArrays within a kernel function. The key difference is that the sizes of these types are known at compile time. If ixx and xy are StaticArrays, then you can do something like θ = ixx*xy. Since the compiler knows the size of ixx and xy, the compiler also know the size of θ. However, even with StaticArrays you must be careful with operations that that create new StaticArrays (like matrix multiplies). These will cause problems if called repeatedly within a loop.2\nIt is possible to dynamicaaly allocate GPU memory within a kernel function, but it requires using the low-level interface to CUDA in CUDA.jl. Moreoever, it is generally not a good idea to be dynamically allocating and freeing memory in each of the thousands of threads you execute.3"
  },
  {
    "objectID": "gpu.html#footnotes",
    "href": "gpu.html#footnotes",
    "title": "Coding for Performance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProcessor registers are the fastest bits of memory on the processor, and registers are where the actual addition, multiplication, and other instructions are carried out.↩︎\nIf you create StaticArrays inside a loop, they get allocated to the GPU’s “dynamic shared memory.” I believe a new allocation happens each loop iteration. This will be slow, and there is a fairly small amount of dynamic shared memory, of which you will soon run out.↩︎\nThere are situations where allocating shared memory is needed and a good idea, but these require some advanced techniques that we will not cover.↩︎"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "ARGridBootstrap",
    "section": "",
    "text": "The notes and examples are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf.\n\nBibTeX citation.\nThe license for the package source code is here."
  }
]