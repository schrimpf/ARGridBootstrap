{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n\nThis work is licensed under a [Creative Commons Attribution-ShareAlike\n4.0 International\nLicense](http://creativecommons.org/licenses/by-sa/4.0/)\n\n\n### About this document {-}\n\nThis document was created using Weave.jl. The code is available in\n[on github](https://github.com/schrimpf/ARGridBootstrap.jl). The same\ndocument generates both static webpages and associated (jupyter\nnotebook)[argridboot.ipynb].\n\n# Introduction\n\nToday we will look into some methods to improve the speed of our\ncode. Although speed is sometimes important, never forget that speed\nshould be low on your list of priorities when writing code. You should\nprioritize correctness and maintainability ahead of\nperformance. Nonetheless, performance does matter for some problems.\n\nIf you have not already, be sure to read [the Peformance Tips section of Julia Docs](https://docs.julialang.org/en/v1/manual/performance-tips/#man-performance-tips-1).\n\nAlso, read Rackauckas's notes on [\"Optimizing Serial Code.\"](https://mitmath.github.io/18337/lecture2/optimizing) [@rackauckas2019a].\n\n# Grid bootstrap\n\nAs a motivating example we will look at the gridded bootstrap of\nHansen (1999)[@hansen99].\n\n<!-- FIXME: DESCRIPTION.  -->\n\nGauss, Matlab, and R code implementing Hansen's method is available on\n[Hansen's\nwebsite](https://www.ssc.wisc.edu/~bhansen/progs/restat_99.html). The\nJulia code below is more or less a direct translation from Hansen's R\ncode. Since this is a translation from R of a translation from Gauss,\nthis code will not necessarily follow best practices for Julia."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using ARGridBootstrap, CodeTracking"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "T = 200\ne = randn(T)\ny0 = 0\na = 0.9\n@code_string b_est_original(e)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@code_string ar1_original(y0,a,e)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@code_string gridbootstrap(b_est_original, a->a, 0.5:0.1:1, 99)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving performance\n\nNow, let's run this code and time it. Note that we are running this\nwith only 50 grid points and 199 bootstrap replications. In real use,\nyou would want more like 999 bootstrap replications or more, and perhaps more\ngrid points."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# simulate some data\nusing Random, BenchmarkTools, Profile\nT = 200\ne = randn(T)\ny0 = 0\na = 0.9\ny = ar1_original(y0, a, e)\nest = b_est_original(y)\nαgrid = 0.84:(0.22/50):1.06\nnboot= 199\nwrapper(b_est) = function(x)\n  out=b_est(x)\n  (out.θ[3], out.se[3])\nend\n@btime (b,t) = gridbootstrap(wrapper(b_est_original), a->ar1_original(y0, a, est.e),\n                             αgrid, nboot);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make code faster, we should begin by profiling."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Profile.clear();\nProfile.init(n=10^7,delay=0.0001);\n@profile (b,t) = gridbootstrap(wrapper(b_est_original), a->ar1_original(y0, a, est.e),\n                               αgrid, 999);\nProfile.print(noisefloor=2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Profile.jl works very simply. Every 0.0001 seconds, the line of code\nbeing executed gets recorded. `Profile.print` shows the count of how\nmany times each line of code got recorded. From the output (these\nnumbers can vary quite a bit from run to run), we see\nthere were 640 ticks in ` gridbootstrap_original` (exact numbers will\nvary on each execution, but relative ones should be similar), and\nalmost all of these occurred within `inv`.  If we want the\ncode to be faster, we should focus on these lines.  Calling both `inv`\nand `\\` is redundant; we should combine these computations."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@code_string b_est_mldivide(y)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime (b,t) = gridbootstrap(wrapper(b_est_mldivide), a->ar1_original(y0, a, est.e),\n                             αgrid, nboot);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this, we get a speedup by about a factor of 4 on my computer."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Profile.clear();\n@profile (b,t) = gridbootstrap(wrapper(b_est_mldivide), a->ar1_original(y0, a, est.e),\n                               αgrid, 999);\nProfile.print(noisefloor=2.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the most time consuming parts of the code are, unsurprisingly,\nthe call to ` \\`, and, perhaps surprisingly, ` hcat` from\ncreating ` x`. Allocating and copying memory is relatively slow. The\ncreation of ` x` involves both. We can avoid creating `x` by just\naccumulating $X'y$ and $X'X$ in a loop."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@code_string b_est_nox(y)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime (b,t) = gridbootstrap(wrapper(b_est_nox), a->ar1_original(y0, a, est.e),\n                             αgrid, nboot);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have further cut the time by a factor of two. However, this performance\noptimization has been costly in terms of readability and extensibility\nof our code. If we wanted to fit an AR(p) model instead of AR(1), the\n` b_est_nox` function would be more difficult to modify than the\n` b_est_mldivide` version.\n\nEXERCISE: Read [the Performance Tips section of Julia\nManual](https://docs.julialang.org/en/v1/manual/performance-tips/) and\nincorporate some of these tips into the above code.\n\nEXERCISE: write a version of ` b_est` that avoids allocating the full\n$T \\times 3$ $X$ matrix, but can still be generalized to an AR(p) model.\n\nEXERCISE: examine how the relative performance of these versions of `\nb_est` vary with ` T`, ` nboot`, and the number of grid points.\n\nEXERCISE: the Julia package ` StaticArrays.jl` provides an alternative\narray implementation that is often much faster than ` Base.Array`. Try\nimplementing ` b_est` using ` StaticArrays.jl`. You will likely need to\nuse mutable arrays (see ` @MMatrix` and ` @MVector`). Note that ` inv` of\na small array will be substantially faster when using ` StaticArray.jl`\ninstead of ` Base.Array`.\n\n## Fastest version\n\nThe fastest version of the code that I could write combines the ideas\nabove. As above, it avoids allocating `x`. It also avoids allocating\n`e` by combining the simulation and estimation into a single\nloop. Finally, it uses mutable static arrays to ensure that operations\non `xx` and `xy` have as little overhead as possible. Note that for\nsmall StaticArrays, `inv` will call a specialized, fast version, and\nends up being faster than `\\`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using StaticArrays"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@code_string simulate_estimate_arp(y0,a,e)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "estimator(y0=y0,e=est.e) = function(a)\n  out = simulate_estimate_arp(y0,a,e)\n  (out.θ[3], out.se[3])\nend\n@btime  (b,t) = gridbootstrap(estimator(), a->a, αgrid, nboot);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "On my computer, this version of the code is about 15 times faster than\nthe original.\n\n\n# Multi-threading\n\nModern computers almost all have multiple cores. We can divide the\ntime it takes our code by up to the number of cores we have (but\nusually less) by writing multi-threaded code. Multi-threaded code\nperforms multiple tasks at once with shared memory. Before you begin\nwriting multi-threaded code, you should make sure your code isn't\nalready using all available cores. It is likely that the BLAS and\nLapack libraries that Julia uses for linear algebra are\nmulti-threaded. If you code is dominated by large matrix operations,\nit may already be using all available cores. In that case, there will\nnot be much benefit from additional multi-threading.\n\nRead [\"The Basics of Single Node Parallel Computing\"](https://mitmath.github.io/18337/lecture5/parallelism_overview)\nRackauckus (2019) [@rackauckus2019b] .\n\nOnce we have decided that the code might benefit from multi-threading,\nwe should look for loops (or other independent tasks) that can be\nmulti-threaded. There is some overhead from creating threads and\ncommunicating among them. Multi-threading generally works best for\nloops where each iteration involves substantial work, and each\niteration is independent of all others. The loops over grid points and\nbootstrap repetitions in ` gridbootstrap` are perfect candidates. We\ndon't care about the order in which these loops get executed. The\nresult of each iteration is (mostly) independent of all others.\n\nSome care must be taken with random number generators and\nmulti-threaded code. See\n[the Julia docs](https://docs.julialang.org/en/v1/manual/parallel-computing/index.html#Side-effects-and-mutable-function-arguments-1) for more information."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "rng = rngarray(nthreads())\n@code_string gridbootstrap_threaded(wrapper(b_est_original),\n                               (a, rng)->ar1_original(y0, a, est.e, n->rand(rng,1:(T-1),n)),\n                               αgrid, 2, rng=rng)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try multi-threading the original version of the code."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Base.Threads\nprintln(\"Single thread, original version\")\n@time begin # this is so slow that using btime is not so necessary\n  (b,t) = gridbootstrap(wrapper(b_est_original), a->ar1_original(y0, a, est.e),\n                        αgrid, 199);\nend;\n\nrng = rngarray(nthreads())\n# make sure the threaded version is compiled before timing it\n(b,t) = gridbootstrap_threaded(wrapper(b_est_original),\n                               (a, rng)->ar1_original(y0, a, est.e, n->rand(rng,1:(T-1),n)),\n                               αgrid, 2, rng=rng);\nprintln(\"$(nthreads()) threads, original version\")\n@time begin # this is so slow that using btime is not so necessary\n  (b,t) = gridbootstrap_threaded(wrapper(b_est_original),\n                                 (a, rng)->ar1_original(y0, a, est.e, n->rand(rng,1:(T-1),n)),\n                                 αgrid, 199, rng=rng);\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The execution times are nearly identical on my computer. The reason is\nthat the computation is dominated by the creation of ` X` and\nmultiplying ` X'*X` and ` X'*y`. These operations are already\nmulti-threaded in the BLAS version I have installed. It is possible\nfirst calling ` using LinearAlgebra; BLAS.set_num_threads(1)` would\nimprove the performance of the multi-threaded bootstrap."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "println(\"Single thread, fastest version\")\n@btime  (b,t) = gridbootstrap(estimator(), a->a, αgrid, nboot);\n\nprintln(\"$(nthreads()) threads, fastest version\")\nestimator_threaded(y0=y0,e=est.e)=function(foo)\n  (a, rng) = foo\n  out=simulate_estimate_arp(y0,a,e,Val(1),()->rand(rng,1:length(e)))\n  (out.θ[3], out.se[3])\nend\nrng = rngarray(nthreads())\n@btime begin\n  (bs, ts) = gridbootstrap_threaded(estimator_threaded(),\n                                    (a,rng)->(a,rng), αgrid,\n                                    nboot, rng=rng)\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the speedup from using multiple threads is far less than\nnumber of cores. On my computer, the threaded version of the code is\nabout 4 times faster, even though my computer has 40 \"cores\" (or 20\nphysical cores. My computer has 2 processors with 10 cores each, and each\ncore is hyperthreaded into 2. The OS sees 40 processors, but half of\nthem are sharing substantial resources). A speedup far less than the\nnumber of cores is typical. Creating and managing multiple threads\ncreates some overhead. Moreover, cores must share various resources;\nmost notably RAM and some cache.\n\n# GPU\n\nCompared to CPUs, GPUs have a huge number of cores operating at a\nslower clockrate. GPUs also have their own separate memory, which they\ncan access faster than CPUs access RAM.  These characteristics make\nGPUs well-suited to large parallel computations. Unfortunately, fully\nutilizing GPUs can require substantial changes to your code.\n\nSee [\"The Different Flavors of\nParallelism\"](https://mitmath.github.io/18337/lecture6/styles_of_parallelism)\nRackauckas (2019) [@rackauckas2019c] for more information comparing\nGPUs to various forms of parallelism on CPUs.\n\n## Array interface\n\nThe easiest way to use a GPU in Julia is through a high level array\ninterface. `ArrayFire.jl`, `oneAPI.jl`, and `CUDA.jl` each offer such\ninterfaces. We will focus on `CUDA.jl` in these\nnotes. `CUDA.jl` relies on Nvidia's CUDA platform, so it only\nworks with Nvidia GPUs. Nvidia tends to dominate GPGPU, and the GPUs\navailable on cedar.computecanada.ca and in my desktop are Nvidia.\n\nUsing CUDA.CuArray is simple, but has some limitations. You create arrays\non the GPU using ` CuArray`. Any array level operation on these will\nthen be performed efficiently on the GPU. This includes broadcast\nfunctions with ` .` and matrix multiplies."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CUDA, Random, BenchmarkTools\n\n\nN = 1000\nM = 1000\n\nfunction cuarraydemo(N,M)\n  # wrapped in a  function so that the CuArrays are freed\n  # otherwise we will run out GPU memory later\n  A = randn(N,M);\n  b = randn(M,2);\n  println(\"Time on CPU\")\n  function foo(A,b)\n    (A.^2)*b\n  end\n  @time c=foo(A,b);\n  @time c=foo(A,b);\n  A_gpu = CuArray(A); # copy of A in GPU memory\n  b_gpu = CuArray(b);\n  println(\"Computations on the GPU are fast\")\n  # @btime does not work inside a function\n  @time CUDA.@sync c_gpu=foo(A_gpu,b_gpu);\n  @time CUDA.@sync c_gpu=foo(A_gpu,b_gpu);\n  println(\"But copying to and from GPU memory is not\")\n  bar(A,b) =Array(foo(CuArray(A), CuArray(b)))\n  @time c2=bar(A,b);\n  @time c2=bar(A,b);\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "cuarraydemo(N,M);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CuArray`s also allow indexing, so you could use loops and other\nconstructs. However, this will not be fast. ` CuArray`s by itself will be\na good method to utilize GPUs when the code is dominated by operations\non large arrays.\n\nUnfortunately, the fastest version of our grid bootstrap code does not\nfit that description. A loop seems needed to generate $y$ due to the\nrecursiveness of the AR(1) model. The fastest version of the code\nabove involves many operations on small 3x3 arrays.\n\nEXERCISE: modify ` b_est_original` or ` b_est_mldivide` to utilize\n` CuArray`s. The approach taken in those functions involves some\nmoderate sized matrices, so it may benefit from ` CuArray`s.\n\n\n## Custom CUDA Kernels\n\nTo parallelize the code above on a GPU, we will have to use a lower\nlevel interface to the GPU. To explain how it works, we will begin\nwith a simple example that just squares all the elements of an array.\n\nDisclaimer: my understanding of CUDA and the inner workings of GPUs is\nfar from complete. Some of the details in this section might be\ninaccurate.\n\nA typical workflow with CUDA consists of\n\n1. Allocate GPU memory and copying arrays into it with ` CuArray`.\n2. Decide how many threads and what configuration of threads to\n   launch.\n3. Each thread does some computation by running a \"kernel\" function.\n4. Copy result from GPU memory to CPU memory.\n\nIn the code below, 1 happens in `cuarray_cudanative_compare`, 2 happens in the\n` square!` function, ` square_kernel!` is the kernel in 3, and 4 is just\nnot done.\n\n### Threads and blocks\n\nCUDA organizes GPU threads into blocks. I believe that the threads in\na block all execute concurrently. Threads in the same block share some\nmemory and registers. All current Nvidia GPUs have a maximum number of\nthreads per block of 1024. Note that threads in the same block share\nregisters[^reg], and different kernel functions will use different\nnumbers of registers at once, so depending on the kernel function, you\nmight be limited to fewer than 1024 threads per block. The number of\nregisters available per block depends on your GPU. You can check your\nGPU characteristics by compiling and running the C++ program in\n`$CUDA_PATH/samples/1_Utilities/deviceQuery/`. Alternatively, you can\nsee this information in Julia by running the code below."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "println(\"Maximum threads per block $(attribute(device(), CUDA.CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK))\")\nprintln(\"Maximum x blocks $(attribute(device(), CUDA.CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X))\")\nprintln(\"Maximum registers per block $(attribute(device(), CUDA.CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK))\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no simple way to predict how many registers a kernel function\nuses. It will depend both on the code you write and how the compiler\noptimizes the code. If you encounter cryptic error messages about CUDA\nresources unavailable, then try reducing the number of threads per\nblock. Alternatively, you can limit the number of registers used by\npassing the `maxregs` argument to `@cuda`.\n\nYou can execute more than 1024 threads by specifying a number of\nblocks. There is also a limit to the number of blocks, but it is\nrather large. In the code below, we set the number of blocks, so that\n` nblocks*nthreads >= length(A)`. Each thread then operates on a single\nelement of ` A`. When the code is executed, each thread has a unique\n` threadIdx` and ` blockIdx` combination, and these are used to assign\nthreads to elements of ` A`. The indices go from 1 to number of threads\n(or blocks). For convenience you can request threads and blocks to\nhave up 3 dimensions, and there are ` threadIdx().y` and\n` threadIdx().z` for the additional dimensions.\n\n[^reg]: Processor registers are the fastest bits of memory on the\n    processor, and registers are where the actual addition,\n    multiplication, and other instructions are carried out."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function square!(A::CuArray)\n  n = length(A)\n  maxthreads = 1024\n  nthreads = min(maxthreads, n)\n  nblocks  = Int(ceil(n/nthreads))\n\n  @cuda threads=nthreads blocks=nblocks square_kernel!(A)\n\n  return A\nend\n\nfunction square_kernel!(A)\n  i = threadIdx().x + (blockIdx().x-1)*blockDim().x\n  if (i<=length(A))\n    @inbounds A[i] *= A[i]\n  end\n  return nothing # CUDA kernels must return nothing\nend\n\nfunction cuarray_cudanative_compare(A)\n  A_gpu = CuArray(A);\n  println(\"CUDAnative square!\")\n  @time CUDA.@sync square!(A_gpu);\n  @time CUDA.@sync square!(A_gpu);\n\n  println(\"CuArray A*=A\")\n  A_gpu = CuArray(A);\n  @time CUDA.@sync A_gpu .*= A_gpu;\n  @time CUDA.@sync A_gpu .*= A_gpu;\n  return nothing\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "cuarray_cudanative_compare(randn(N,M))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kernel Limitations\n\nCUDA kernel functions execute on the GPU and in GPU memory. Since GPU\nmemory is allocated and managed differently than RAM, many Julia\nfunctions will not work in CUDA kernels. Most importantly, Julia\nfunctions that allocate dynamically sized arrays will not work. This\nmeans that even matrix multiplication like ` θ = ixx*xy` will fail (if\n`ixx` or `xy` are dynamically allocated) since it allocates an array\nfor ` θ`. You can, however, have local scalars, tuples, and `\nStaticArrays` within a kernel function. The key difference is that the\nsizes of these types are known at compile time. If `ixx` and `xy` are\n`StaticArrays`, then you can do something like `θ = ixx*xy`. Since the\ncompiler knows the size of `ixx` and `xy`, the compiler also know the\nsize of `θ`. However, even with ` StaticArrays` you must be careful\nwith operations that that create new StaticArrays (like matrix\nmultiplies). These will cause problems if called repeatedly within a\nloop.[^loops]\n\n[^loops]: If you create StaticArrays inside a loop, they get allocated\n    to the GPU's \"dynamic shared memory.\" I believe a new allocation\n    happens each loop iteration. This will be slow, and there is a\n    fairly small amount of dynamic shared memory, of which you will\n    soon run out.\n\nIt is possible to dynamicaaly allocate GPU memory within a kernel\nfunction, but it requires using the low-level interface to CUDA in\n`CUDA.jl`. Moreoever, it is generally not a good idea to be\ndynamically allocating and freeing memory in each of the thousands of\nthreads you execute.[^caveat]\n\n\n[^caveat]: There are situations where allocating shared memory is\n    needed and a good idea, but these require some advanced techniques\n    that we will not cover.\n\n## GPU grid bootstrap"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@code_string argridbootstrap_gpu(est.e, y0, grid=αgrid, nboot=nboot, RealType=Float64)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@code_string ARGridBootstrap.argridkernel!(1.,1., 1., Val(1), 1., 1. , 1.)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime begin\n  grid = argridbootstrap_gpu(est.e, y0, grid=αgrid, nboot=nboot, RealType=Float64);\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to the fastest CPU code above, the GPU version takes about\n1/20th the time of the single-threaded CPU code, and about 1/5th the\ntime of the 30-threaded CPU code. Considering that the two CPUs in my\nworkstation together cost about 6 times more than the single GPU, the\nperformance of the GPU code is quite good. Also, we carefully profiled\nand tuned the CPU code, but not the GPU code (although the GPU code\ndoes use all algorithmic improvements of the fastest CPU code). Profiling GPU kernel\ncode requires using Nvidia's profiler, see\n[CUDA.jl\ndocumentation](https://cuda.juliagpu.org/stable/development/profiling/)\nfor information."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.7.2"
    },
    "kernelspec": {
      "name": "julia-1.7",
      "display_name": "Julia 1.7.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
