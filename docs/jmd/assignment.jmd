---
title       : "Coding for Performance"
subtitle    : "4: More Examples"
author      : Paul Schrimpf
date        : `j using Dates; print(Dates.today())`
bibliography: "perf.bib"
---

For the assignment, most students focused on optimizing either the [`share` function](https://github.com/ubcecon/ECON622_BLP.jl/blob/01fbfd6547ba789b38ccc7252de38a0e2f4188ee/src/blp.jl#L24) from our earlier BLP example,
or the [`statparts` function from GMMInference](https://github.com/schrimpf/GMMInference.jl/blob/9f850d176001e9650b24fe33a7bf14692a817c69/src/HypothesisTests/CLR_KLM.jl#L32).

Let's see how we can improve these functions' performance.

# `share`

Here's a copy of the original version of the share function

```julia
using LinearAlgebra, Distributions, Random, BenchmarkTools, Profile, ProfileCanvas

function share(δ, Σ, x, ∫)
  J,K = size(x)
  (length(δ) == J) || error("length(δ)=$(length(δ)) != size(x,1)=$J")
  (K,K) == size(Σ) || error("size(x,2)=$K != size(Σ)=$(size(Σ))")
  function shareν(ν)
    s = δ .+ x*Σ*ν
    smax=max(0,maximum(s))
    s .-= smax
    s .= exp.(s)
    s ./= (sum(s) + exp(0-smax))
    return(s)
  end
  return(∫(shareν))
end
```
and a simple function for the integral.
```julia
struct Integrator{Tx, Tw}
    x::Tx
    w::Tw
end

function Integrator(dx::Distribution, n=100)
    x = [rand(dx) for _ in 1:n]
    w = Base.Iterators.Repeated(1/n)
    Integrator(x,w)
end

(∫::Integrator)(f) = sum((xw)->f(xw[1])*xw[2], zip(∫.x, ∫.w))
```
For simplicity and to focus on the share function, we just use Monte Carlo integration since its easy to implement, but we could get more accuracy by using quasi monte carlo or quadrature.

## Initial Benchmark


```julia; results="hidden"
J = 10
K = 5
δ = rand(J)
X = randn(J,K)
Σ = I + zeros(K,K)
∫ = Integrator(MvNormal(zeros(K),I));
```

```julia; cache=true
@benchmark share(δ,Σ,X,∫)
```

```julia; echo=false; results="hidden"
using ProfileCanvas
function profilehtmlstring()
  buf = IOBuffer()
  show(buf, MIME("text/html"), ProfileCanvas.view(Profile.fetch()))
  s=String(take!(buf))
  println("\n\n"*s*"\n")
end
```
Profiling it:

```julia; cache=true; results="raw"
using Profile
Profile.clear();
Profile.init(n=10^7,delay=0.00001);
@profile sum(share(δ,Σ,X,∫) for i ∈ 1:1000)
profilehtmlstring()
```

It's a good idea to check `@code_warntype` and verify that there are no type instabilities (variables with type `Any` or `Union`). I checked and there are no type instabilites here.

## LoopVectorization

Examining the profile, we see that the multiplication of `x*Σ*ν` is the single most costly operation.
The second most costly are lines computing the softmax function, `exp.(s)./(1 .+exp.(s))`

```julia; cache=true
using LoopVectorization

@inline function dplusxMy(d::AbstractVector,x::AbstractMatrix,M::AbstractMatrix,y::AbstractVector)
  out = similar(d)
  @turbo for i ∈ axes(x,1)
    r = d[i]
    for j ∈ axes(x,2)
      for k ∈ axes(M,2)
        r += x[i,j]*M[j,k]*y[k]
      end
    end
    out[i] = r
  end
  out
end
@inline function softmax0!(s)
  smax=@turbo reduce(max, s, init=zero(eltype(s)))
  s .= exp.(s .- smax)
  s ./= (sum(s) + exp(zero(smax)-smax))
  return(s)
end
function share_v2(δ, Σ, x, ∫)
  J,K = size(x)
  (length(δ) == J) || error("length(δ)=$(length(δ)) != size(x,1)=$J")
  (K,K) == size(Σ) || error("size(x,2)=$K != size(Σ)=$(size(Σ))")
  function shareν(ν)
    s = dplusxMy(δ,x,Σ,ν)
    softmax0!(s)
    s
  end
  return(∫(shareν))
end
@benchmark share_v2(δ,Σ,X,∫)
```

By writing the `δ+x*Σ*ν` as loop we avoid some allocations, and can use `@turbo` to insert SIMD instructions. This gives a
noticeable speedup. About half the gains here are from writing out the loops in `dplusxMy` and half from using `@turbo`. The changes
to the softmax calculation did not make much difference.

```julia; cache=true; results="raw"
using Profile
Profile.clear();
Profile.init(n=10^7,delay=0.00001);
ProfileCanvas.@profview sum(share_v2(δ,Σ,X,∫) for i ∈ 1:1000)
profilehtmlstring()
```

The execution time is still dominated by the two functions mentioned above.

## StaticArrays

Since the calculations involve small arrays, it is likely to benefit from using `StaticArrays`.

```julia; cache=true
using StaticArrays
sδ = SVector{J}(δ)
sΣ = SMatrix{K,K}(Σ)
sX = SMatrix{J,K}(X)
nd = length(∫.x)
iw = SVector{nd}(fill(1/nd,nd))
ix = [SVector{K}(x) for x ∈ ∫.x]
s∫ = Integrator(ix,iw)
@benchmark share_v2(sδ,sΣ,sX,s∫)
```

The code is now 2 times faster than what we started with.

Interestly, slightly better performance can be achieved by simply passing `StaticArrays` to the original code.

```julia
@benchmark share($(MVector(sδ)),sΣ,sX,s∫) # δ gets mutated, so we must make it a Mutable SArray.
```

## Non-allocating

We can do still better by eliminating the allocations. With normal arrays, broadcast operations are usually non-allocating, and normal array operations allocate.
With `StaticArrays`, normal operatorations do not allocate.^[Like scalar variables, StaticArrays exist on the stack instead of the heap, so creating them is much less costly and they do not count toward the reported allocations.]

Additionally, we can precompute x*Σ outside of `shareν` (we could have done this at any time earlier too). This change accounts for about 1μs of the speed up.
```julia
function share_v3(δ, Σ, x, ∫)
  J,K = size(x)
  (length(δ) == J) || error("length(δ)=$(length(δ)) != size(x,1)=$J")
  (K,K) == size(Σ) || error("size(x,2)=$K != size(Σ)=$(size(Σ))")
  xΣ = x*Σ
  function shareν(ν)
    s = δ + xΣ*ν
    smax=max(0,maximum(s))
    s -= smax*ones(typeof(s))
    s = exp.(s)
    s *= 1/(sum(s) + exp(0-smax))
    return(s)
  end
  return(∫(shareν))
end
```

```julia
@benchmark share_v3(sδ,sΣ,sX,s∫)
```

The code is now 5 times faster than the original, and only allocates once (to store the return value). 

## Multi-Threading

Multi-threading this code is difficult because it is already quite fast. The sum in the integral can be parallelized, but unless 
there are a large number of integration points, the overhead from creating threads will likely outweigh the benefits. 

The [`Polyester` package](https://github.com/JuliaSIMD/Polyester.jl) provides a faster, but more limited threading model than base Julia. 
Using it, we can see modest gains, even with just 100 integration points. 

To get type stability, I had to write the integration sum loop inside the share function. 
It would have been cleaner to keep the integration sum in a separate function, but I could not make it type stable that way.


```julia
import Polyester
function share_v4(δ, Σ, x, ∫, ::Val{B}=Val(length(∫.x) ÷ 10)) where {B}
  J,K = size(x)
  (length(δ) == J) || error("length(δ)=$(length(δ)) != size(x,1)=$J")
  (K,K) == size(Σ) || error("size(x,2)=$K != size(Σ)=$(size(Σ))")
  xΣ = x*Σ
  function shareν(ν)
    s = δ + xΣ*ν
    smax=max(0,maximum(s))
    s -= smax*ones(typeof(s))
    s = exp.(s)
    s *= 1/(sum(s) + exp(0-smax))
    return(s)
  end
  batchlen= length(∫.x)÷B
  @assert B*batchlen==length(∫.x)
  out = MVector{B,typeof(δ)}(undef)
  Polyester.@batch for b ∈ 1:B  
    batch = ((b-1)*(batchlen)+1):(b*batchlen)
    out[b] = zero(typeof(δ))
    for i ∈ batch
      out[b] += shareν(∫.x[i])*∫.w[i]
    end
  end
  return(sum(out))
end
Polyester.reset_threads!()
@benchmark share_v4(sδ,sΣ,sX,s∫, Val(20))
```

# `statparts`

A few people worked on optimizing the `klm` or `clr` function from [GMMInference.jl](https://github.com/schrimpf/GMMInference.jl).
Here is the original code for `klm`.
```julia
using ForwardDiff
function statparts(gi::Function)
  function P(A::AbstractMatrix) # projection matrix
    A*pinv(A'*A)*A'
  end
  function(θ)
    giθ = gi(θ)
    p = length(θ)
    (n, k) = size(giθ)
    Ω = cov(giθ)
    gn=mean(gi(θ), dims=1)'
    Gi= ForwardDiff.jacobian(gi,θ)
    Gi = reshape(Gi, n , k, p)
    G = mean(Gi, dims=1)
    Γ = zeros(eltype(Gi),p,k,k)
    D = zeros(eltype(Gi),k, p)
    for j in 1:p
      for i in 1:n
        Γ[j,:,:] += (Gi[i,:,j] .- G[1,:,j]) * giθ[i,:]'
      end
      Γ[j,:,:] ./= n
      D[:,j] = G[1,:,j] - Γ[j,:,:]*inv(Ω)*gn
    end
    return(n,k,p,gn, Ω, D, P)
  end
end

function klm(gi::Function)
  SP = statparts(gi)
  function(θ)
    (n,k,p,gn, Ω, D, P) = SP(θ)
    return n*(gn'*Ω^(-1/2)*P(Ω^(-1/2)*D)*Ω^(-1/2)*gn)[1]
  end
end
```
To run the code, we need an example `gi` function. We'll just copy the example from [the docs](https://schrimpf.github.io/GMMInference.jl/dev/identificationRobustInference/#Example:-IV-logit-demand).
```julia
import Random
function simulate_ivshare(n,β,γ,ρ)
  z = randn(n, size(γ)[1])
  endo = randn(n, length(β))
  x = z*γ .+ endo
  ξ = rand(Normal(0,sqrt((1.0-ρ^2))),n).+endo[:,1]*ρ
  y = cdf.(Logistic(), x*β .+ ξ)
  return((y=y,x=x,z=z))
end
n = 100
k = 2
iv = 3
β0 = ones(k)
π0 = vcat(5*I,ones(iv-k,k))
ρ = 0.5
Random.seed!(622)
(y,x,z) = simulate_ivshare(n,β0,π0,ρ)

function gi_ivshare(β,y,x,z)
  ξ = quantile.(Logistic(),y) .- x*β
  ξ.*z
end

gi = let y=y, x=x, z=z
  β->gi_ivshare(β,y,x,z)
end
```

## Initial Benchmark
```julia
@benchmark klm(gi)(β0)
```

## Fixing Type Instabilities

From `@code_warntype`, we see that the compiles is unable to infer the type of some variables. The problem seems to start with `D`.  This is quite puzzling because `D` is explicitly initialized as `zeros(eltype(Gi),...).`
```julia; term=true
@code_warntype klm(gi)(β0)
```
To investigate further, let us focus on `statparts`.

```julia; term=true
@code_warntype statparts(gi)(β0)
```
We see that `G`, `Gi`, `Γ`, and `D` are all type `Any`.  For some reason, the return value of `ForwardDiff.jacobian` is not being inferred. We can workaround this by using an `jacobian!` instead.

```julia
function statparts(gi::F) where {F <: Function}
  function P(A::AbstractMatrix) # projection matrix
    A*pinv(A'*A)*A'
  end
  let gi=gi
    function(θ)
      giθ = gi(θ)
      p = length(θ)
      (n, k) = size(giθ)
      Ω = Hermitian(cov(giθ))
      gn=mean(gi(θ), dims=1)'
      Gi = zeros(n,k,p)
      ForwardDiff.jacobian!(Gi,gi,θ)
      Gi = reshape(Gi, n , k, p)
      G = mean(Gi, dims=1)
      Γ = zeros(eltype(Gi),p,k,k)
      D = zeros(eltype(Gi),k, p)
      for j in 1:p
        for i in 1:n
          Γ[j,:,:] += (Gi[i,:,j] .- G[1,:,j]) * giθ[i,:]'
        end
        Γ[j,:,:] ./= n
        D[:,j] = G[1,:,j] - Γ[j,:,:]*inv(Ω)*gn
      end
      return(n,k,p,gn, Ω, D, P)
    end
  end
end
```

```julia; term=true
@code_warntype statparts(gi)(β0)
```

I also added the `where {F` statement to [ensure compiler specialization](https://docs.julialang.org/en/v1/manual/performance-tips/#Be-aware-of-when-Julia-avoids-specializing), and
added the `let gi=gi` line to help [with the performance of captured variables](https://docs.julialang.org/en/v1/manual/performance-tips/#man-performance-captured).

```julia; term=true
@code_warntype klm(gi)(β0)
```
There's still a type-instability in `klm`. This one is harder to understand. It is due to the fact that the appropriate meaning of a matrix square root depends on the nature of the matrix.
In particular, the value could be a complex valued matrix instead of real valued.
We know that Ω should be positive definite with a real matrix square root. We can compute its square root from its Eigen decomposition and avoid the type instability.

```julia
function klm(gi::F ) where {F <: Function}
  let gi=gi
    function(θ)
      (n,k,p,gn, Ω, D, P) = statparts(gi)(θ)
      λ, v = eigen(Ω)
      irΩ = v*diagm(λ.^(-1/2))*v'
      return n*(gn'*irΩ*P(irΩ*D)*irΩ*gn)[1]
    end
  end
end
```

```julia; term=true
@code_warntype klm(gi)(β0)
```

Fixing these type instabilities speeds up the code by a factor of about 5.

```julia
@benchmark klm(gi)(β0)
```

## Reducing allocations and Other Optimizations

Profiling reveals the majority of time is spent in the innermost loop of the `statparts` function.
This loop allocates quite a bit because the arrays are using slices. We can avoid allocations by using `@views` and more broadcasting.
See ["Consider using views for slices"](https://docs.julialang.org/en/v1/manual/performance-tips/#man-performance-views) and
["More dots"](https://docs.julialang.org/en/v1/manual/performance-tips/#More-dots:-Fuse-vectorized-operations),

```julia
function statparts(gi::F) where {F <: Function}
  function P(A::AbstractMatrix) # projection matrix
    A*pinv(A'*A)*A'
  end
  let gi=gi
    function(θ)
      giθ = gi(θ)
      p = length(θ)
      (n, k) = size(giθ)
      Ω = Hermitian(cov(giθ))
      gn=mean(gi(θ), dims=1)'
      iΩgn = Ω \ gn
      Gi = zeros(n,k,p)
      ForwardDiff.jacobian!(Gi,gi,θ)
      Gi = reshape(Gi, n , k, p)
      G = mean(Gi, dims=1)
      Γ = zeros(eltype(Gi),p,k,k)
      D = zeros(eltype(Gi),k, p)
      @inbounds for j in 1:p
        @inbounds for i in 1:n
          @views Γ[j,:,:] .+= (Gi[i,:,j] .- G[1,:,j]) * giθ[i,:]'
        end
        Γ[j,:,:] ./= n
        @views D[:,j] .= G[1,:,j] .- Γ[j,:,:]*iΩgn
      end
      return(n,k,p,gn, Ω, D, P)
    end
  end
end
@benchmark klm(gi)(β0)
```

The code is now about ten times faster than the original.
